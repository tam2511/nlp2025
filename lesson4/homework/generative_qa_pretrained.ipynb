{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative QA with Pretrained QA Encoder\n",
    "\n",
    "–ù–æ—É—Ç–±—É–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π QA —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –Ω–∞ SQuAD encoder** –∏ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ decoder.\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è**: \n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º DistilBERT, fine-tuned –Ω–∞ SQuAD (distilbert-base-cased-distilled-squad)\n",
    "- –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º encoder –∏ **–ø—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ–º** –≤—Å–µ embeddings –∑–∞—Ä–∞–Ω–µ–µ\n",
    "- Decoder –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **pretrained embeddings** –æ—Ç DistilBERT\n",
    "- –û–±—É—á–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π Transformer Decoder (256-dim, 2 —Å–ª–æ—è, 8 –≥–æ–ª–æ–≤)\n",
    "\n",
    "**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**:\n",
    "- Encoder max length: 384 —Ç–æ–∫–µ–Ω–æ–≤ (question + context)\n",
    "- Decoder max length: 32 —Ç–æ–∫–µ–Ω–∞ (–æ—Ç–≤–µ—Ç)\n",
    "- –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ encoder embeddings (—É—Å–∫–æ—Ä–µ–Ω–∏–µ ~10x)\n",
    "- –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π decoder —Å projection layer\n",
    "- **Pretrained embeddings** ‚Äî decoder —Å—Ä–∞–∑—É –∑–Ω–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 1: –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "pl.seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ DistilBERT (fine-tuned –Ω–∞ SQuAD) –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained QA model: distilbert-base-cased-distilled-squad\n",
      "Pretrained embeddings shape: torch.Size([28996, 768])\n",
      "Encoder loaded and frozen\n",
      "Vocab size: 28996\n",
      "Encoder hidden size: 768\n",
      "\n",
      "Loading SQuAD dataset...\n",
      "Train examples: 87599\n",
      "Val examples: 10570\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT fine-tuned –Ω–∞ SQuAD\n",
    "qa_model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "print(f\"Loading pretrained QA model: {qa_model_name}\")\n",
    "qa_encoder = AutoModel.from_pretrained(qa_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º pretrained embeddings –¥–ª—è decoder\n",
    "pretrained_embeddings = qa_encoder.embeddings.word_embeddings.weight.clone().detach()\n",
    "print(f\"Pretrained embeddings shape: {pretrained_embeddings.shape}\")\n",
    "\n",
    "# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º encoder\n",
    "for param in qa_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "qa_encoder.eval()\n",
    "qa_encoder.to(device)\n",
    "\n",
    "ENCODER_HIDDEN_SIZE = qa_encoder.config.hidden_size  # 768\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Encoder loaded and frozen\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Encoder hidden size: {ENCODER_HIDDEN_SIZE}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π SQuAD –¥–∞—Ç–∞—Å–µ—Ç\n",
    "print(\"\\nLoading SQuAD dataset...\")\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "\n",
    "# –ü–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "train_contexts = train_data['context']\n",
    "train_questions = train_data['question']\n",
    "train_answers = train_data['answers']\n",
    "\n",
    "val_contexts = val_data['context']\n",
    "val_questions = val_data['question']\n",
    "val_answers = val_data['answers']\n",
    "\n",
    "print(f\"Train examples: {len(train_contexts)}\")\n",
    "print(f\"Val examples: {len(val_contexts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 3: –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ encoder embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precompute function defined\n"
     ]
    }
   ],
   "source": [
    "MAX_ENCODER_LENGTH = 384  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤\n",
    "MAX_DECODER_LENGTH = 32\n",
    "\n",
    "def precompute_encoder_embeddings(\n",
    "    contexts, questions, answers, encoder, tokenizer,\n",
    "    max_length=256, max_decoder_length=32, batch_size=64, device='cuda'\n",
    "):\n",
    "    encoder.eval()\n",
    "    \n",
    "    encoder_embeddings = []\n",
    "    encoder_masks = []\n",
    "    decoder_inputs = []\n",
    "    decoder_targets = []\n",
    "    decoder_masks = []\n",
    "    answer_texts = []\n",
    "    \n",
    "    num_examples = len(contexts)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in tqdm(range(0, num_examples, batch_size), desc=\"Precomputing\"):\n",
    "            end_idx = min(start_idx + batch_size, num_examples)\n",
    "            \n",
    "            batch_questions = questions[start_idx:end_idx]\n",
    "            batch_contexts = contexts[start_idx:end_idx]\n",
    "            batch_answers = answers[start_idx:end_idx]\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                list(batch_questions), list(batch_contexts),\n",
    "                max_length=max_length, padding='max_length',\n",
    "                truncation=True, return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            \n",
    "            for i in range(end_idx - start_idx):\n",
    "                encoder_embeddings.append(hidden_states[i].cpu())\n",
    "                encoder_masks.append(attention_mask[i].cpu())\n",
    "                \n",
    "                answer_text = batch_answers[i]['text'][0]\n",
    "                answer_texts.append(answer_text)\n",
    "                \n",
    "                answer_tokens = tokenizer.encode(answer_text, add_special_tokens=False)\n",
    "                answer_tokens = answer_tokens[:max_decoder_length - 2]\n",
    "                \n",
    "                decoder_input = [tokenizer.cls_token_id] + answer_tokens\n",
    "                decoder_target = answer_tokens + [tokenizer.sep_token_id]\n",
    "                \n",
    "                pad_len = max_decoder_length - len(decoder_input)\n",
    "                decoder_input = decoder_input + [tokenizer.pad_token_id] * pad_len\n",
    "                decoder_target = decoder_target + [tokenizer.pad_token_id] * pad_len\n",
    "                decoder_mask = [1] * (len(answer_tokens) + 1) + [0] * pad_len\n",
    "                \n",
    "                decoder_inputs.append(torch.tensor(decoder_input, dtype=torch.long))\n",
    "                decoder_targets.append(torch.tensor(decoder_target, dtype=torch.long))\n",
    "                decoder_masks.append(torch.tensor(decoder_mask, dtype=torch.long))\n",
    "    \n",
    "    return {\n",
    "        'encoder_embeddings': encoder_embeddings,\n",
    "        'encoder_masks': encoder_masks,\n",
    "        'decoder_inputs': decoder_inputs,\n",
    "        'decoder_targets': decoder_targets,\n",
    "        'decoder_masks': decoder_masks,\n",
    "        'answer_texts': answer_texts\n",
    "    }\n",
    "\n",
    "print(\"Precompute function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing train embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c056471fee47f28973395d9afd1569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precomputing:   0%|          | 0/1369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precomputing val embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c770c9c48ff4db9b368ed29b75e28f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precomputing:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train cache: 87599 examples\n",
      "Val cache: 10570 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Precomputing train embeddings...\")\n",
    "train_cache = precompute_encoder_embeddings(\n",
    "    train_contexts, train_questions, train_answers,\n",
    "    qa_encoder, tokenizer,\n",
    "    max_length=MAX_ENCODER_LENGTH,\n",
    "    max_decoder_length=MAX_DECODER_LENGTH,\n",
    "    batch_size=64, device=device\n",
    ")\n",
    "\n",
    "print(\"\\nPrecomputing val embeddings...\")\n",
    "val_cache = precompute_encoder_embeddings(\n",
    "    val_contexts, val_questions, val_answers,\n",
    "    qa_encoder, tokenizer,\n",
    "    max_length=MAX_ENCODER_LENGTH,\n",
    "    max_decoder_length=MAX_DECODER_LENGTH,\n",
    "    batch_size=64, device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain cache: {len(train_cache['encoder_embeddings'])} examples\")\n",
    "print(f\"Val cache: {len(val_cache['encoder_embeddings'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 4: Dataset —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 87599\n",
      "Val dataset: 10570\n"
     ]
    }
   ],
   "source": [
    "class CachedEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, cache):\n",
    "        self.encoder_embeddings = cache['encoder_embeddings']\n",
    "        self.encoder_masks = cache['encoder_masks']\n",
    "        self.decoder_inputs = cache['decoder_inputs']\n",
    "        self.decoder_targets = cache['decoder_targets']\n",
    "        self.decoder_masks = cache['decoder_masks']\n",
    "        self.answer_texts = cache['answer_texts']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoder_embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'encoder_hidden_states': self.encoder_embeddings[idx],\n",
    "            'encoder_attention_mask': self.encoder_masks[idx],\n",
    "            'decoder_input_ids': self.decoder_inputs[idx],\n",
    "            'decoder_attention_mask': self.decoder_masks[idx],\n",
    "            'decoder_target_ids': self.decoder_targets[idx],\n",
    "            'answer_text': self.answer_texts[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = CachedEmbeddingsDataset(train_cache)\n",
    "val_dataset = CachedEmbeddingsDataset(val_cache)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Val dataset: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 5: –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π Decoder —Å Pretrained Embeddings\n",
    "\n",
    "- d_model=256, 2 —Å–ª–æ—è, 8 –≥–æ–ª–æ–≤\n",
    "- Pretrained embeddings –æ—Ç DistilBERT (768-dim) ‚Üí projection ‚Üí 256-dim\n",
    "- Projection layer –¥–ª—è encoder output: 768 ‚Üí 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compact Decoder with Pretrained Embeddings\n",
      "Total parameters: 31,696,196 (~31.7M)\n",
      "Trainable parameters: 31,696,196 (~31.7M)\n"
     ]
    }
   ],
   "source": [
    "DECODER_DIM = 256\n",
    "DECODER_HEADS = 8\n",
    "DECODER_LAYERS = 2  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–≥–æ encoder\n",
    "DECODER_FF = 512\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
    "        tgt2 = self.cross_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
    "        tgt2 = self.linear2(self.dropout(F.gelu(self.linear1(tgt))))\n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
    "        return tgt\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "class CompactDecoderQA(nn.Module):\n",
    "    \"\"\"Decoder —Å pretrained embeddings.\n",
    "    \n",
    "    - Pretrained embeddings (768-dim) -> projection -> 256-dim\n",
    "    - Encoder output (768-dim) -> projection -> 256-dim\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, pretrained_embeddings, encoder_dim=768, decoder_dim=256, \n",
    "                 nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.decoder_dim = decoder_dim\n",
    "        embed_dim = pretrained_embeddings.shape[1]  # 768\n",
    "        \n",
    "        # Pretrained embeddings (frozen) + projection\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.embed_projection = nn.Linear(embed_dim, decoder_dim)\n",
    "        \n",
    "        # Encoder output projection\n",
    "        self.encoder_projection = nn.Linear(encoder_dim, decoder_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(decoder_dim, dropout)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.layers = ...\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(decoder_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_hidden_states, encoder_attention_mask, decoder_input_ids, decoder_attention_mask=None):\n",
    "        # Project encoder output: [batch, seq, 768] -> [batch, seq, 256]\n",
    "        ...\n",
    "        \n",
    "        # Embed and project decoder input: [batch, seq] -> [batch, seq, 768] -> [batch, seq, 256]\n",
    "        x = self.embedding(decoder_input_ids)\n",
    "        ...\n",
    "        \n",
    "        # Causal mask\n",
    "        seq_len = decoder_input_ids.size(1)\n",
    "        ...\n",
    "        \n",
    "        # Padding masks\n",
    "        memory_key_padding_mask = (encoder_attention_mask == 0)\n",
    "        tgt_key_padding_mask = (decoder_attention_mask == 0) if decoder_attention_mask is not None else None\n",
    "        \n",
    "        # Decoder layers apply\n",
    "        ...\n",
    "        \n",
    "        return ...\n",
    "\n",
    "# –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "test_model = CompactDecoderQA(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    encoder_dim=ENCODER_HIDDEN_SIZE,\n",
    "    decoder_dim=DECODER_DIM,\n",
    "    nhead=DECODER_HEADS,\n",
    "    num_layers=DECODER_LAYERS,\n",
    "    dim_feedforward=DECODER_FF\n",
    ")\n",
    "num_params = sum(p.numel() for p in test_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "print(f\"Compact Decoder with Pretrained Embeddings\")\n",
    "print(f\"Total parameters: {num_params:,} (~{num_params/1e6:.1f}M)\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} (~{trainable_params/1e6:.1f}M)\")\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 6: Lightning –º–æ–¥—É–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning module defined\n"
     ]
    }
   ],
   "source": [
    "class CompactDecoderQALightning(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, pretrained_embeddings, encoder_dim=768, decoder_dim=256, \n",
    "                 nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1, lr=3e-4, warmup_steps=500):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['pretrained_embeddings'])\n",
    "        \n",
    "        self.model = CompactDecoderQA(\n",
    "            vocab_size=vocab_size,\n",
    "            pretrained_embeddings=pretrained_embeddings,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        \n",
    "    def forward(self, encoder_hidden_states, encoder_attention_mask, decoder_input_ids, decoder_attention_mask):\n",
    "        return self.model(encoder_hidden_states, encoder_attention_mask, decoder_input_ids, decoder_attention_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch['encoder_hidden_states'], batch['encoder_attention_mask'],\n",
    "                      batch['decoder_input_ids'], batch['decoder_attention_mask'])\n",
    "        loss = self.criterion(logits.view(-1, self.vocab_size), batch['decoder_target_ids'].view(-1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self(batch['encoder_hidden_states'], batch['encoder_attention_mask'],\n",
    "                      batch['decoder_input_ids'], batch['decoder_attention_mask'])\n",
    "        loss = self.criterion(logits.view(-1, self.vocab_size), batch['decoder_target_ids'].view(-1))\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < self.warmup_steps:\n",
    "                return float(step) / float(max(1, self.warmup_steps))\n",
    "            return max(0.0, float(total_steps - step) / float(max(1, total_steps - self.warmup_steps)))\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n",
    "\n",
    "print(\"Lightning module defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 7: –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA L40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 685\n",
      "Val batches: 83\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | CompactDecoderQA | 31.7 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "31.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.7 M    Total params\n",
      "126.785   Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de261ea35d54270b1c1c635ebe92305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 47. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 74. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Model\n",
    "lightning_model = CompactDecoderQALightning(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    "    encoder_dim=ENCODER_HIDDEN_SIZE,\n",
    "    decoder_dim=DECODER_DIM,\n",
    "    nhead=DECODER_HEADS,\n",
    "    num_layers=DECODER_LAYERS,\n",
    "    dim_feedforward=DECODER_FF,\n",
    "    dropout=0.1,\n",
    "    lr=3e-4,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=50,\n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.fit(lightning_model, train_loader, val_loader)\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation:\n",
      "\n",
      "============================================================\n",
      "Ground Truth: Denver Broncos\n",
      "Generated:    Vienna Literary Agreement\n",
      "------------------------------------------------------------\n",
      "Ground Truth: Carolina Panthers\n",
      "Generated:    Detroit Newspaper\n",
      "------------------------------------------------------------\n",
      "Ground Truth: Santa Clara, California\n",
      "Generated:    Jan ' s Park\n",
      "------------------------------------------------------------\n",
      "Ground Truth: Denver Broncos\n",
      "Generated:    Boston Globe\n",
      "------------------------------------------------------------\n",
      "Ground Truth: gold\n",
      "Generated:    yellow\n",
      "------------------------------------------------------------\n",
      "Ground Truth: \"golden anniversary\"\n",
      "Generated:    the golden age\n",
      "------------------------------------------------------------\n",
      "Ground Truth: February 7, 2016\n",
      "Generated:    February 7, 2016\n",
      "------------------------------------------------------------\n",
      "Ground Truth: American Football Conference\n",
      "Generated:    National Football League\n",
      "------------------------------------------------------------\n",
      "Ground Truth: \"golden anniversary\"\n",
      "Generated:    the golden age\n",
      "------------------------------------------------------------\n",
      "Ground Truth: American Football Conference\n",
      "Generated:    American Football League\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(decoder_model, encoder_hidden, encoder_mask, tokenizer, max_length=32):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è greedy decoding.\"\"\"\n",
    "    decoder_model.eval()\n",
    "    device = encoder_hidden.device\n",
    "    decoder_ids = torch.tensor([[tokenizer.cls_token_id]], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            decoder_mask = torch.ones_like(decoder_ids)\n",
    "            logits = decoder_model(encoder_hidden, encoder_mask, decoder_ids, decoder_mask)\n",
    "            next_token = logits[0, -1, :].argmax().unsqueeze(0).unsqueeze(0)\n",
    "            decoder_ids = torch.cat([decoder_ids, next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(decoder_ids[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é\n",
    "lightning_model.eval()\n",
    "device = next(lightning_model.parameters()).device\n",
    "\n",
    "print(\"Testing generation:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(10):\n",
    "    sample = val_dataset[i]\n",
    "    encoder_hidden = sample['encoder_hidden_states'].unsqueeze(0).to(device)\n",
    "    encoder_mask = sample['encoder_attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    generated = generate_answer(lightning_model.model, encoder_hidden, encoder_mask, tokenizer)\n",
    "    \n",
    "    print(f\"Ground Truth: {sample['answer_text']}\")\n",
    "    print(f\"Generated:    {generated}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 9: –ü–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
    "\n",
    "–í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è QA:\n",
    "- **Exact Match (EM)** ‚Äî —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ ground truth\n",
    "- **F1 Score** ‚Äî F1 –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric functions defined\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞: lowercase, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –∞—Ä—Ç–∏–∫–ª–µ–π.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Exact Match: 1 –µ—Å–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∏–Ω–∞—á–µ 0.\"\"\"\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"F1 Score –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤.\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return int(pred_tokens == gt_tokens)\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "print(\"Metric functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1000 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6996d04c677499fbd926b1fc944995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–û–ú –ù–ê–ë–û–†–ï\n",
      "==================================================\n",
      "Exact Match (EM): 17.00%\n",
      "F1 Score:         27.97%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "lightning_model.eval()\n",
    "device = next(lightning_model.parameters()).device\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ–¥–ª–µ–Ω–Ω–∞—è)\n",
    "EVAL_SAMPLES = 1000\n",
    "eval_indices = random.sample(range(len(val_dataset)), min(EVAL_SAMPLES, len(val_dataset)))\n",
    "\n",
    "print(f\"Evaluating on {len(eval_indices)} samples...\")\n",
    "for i in tqdm(eval_indices):\n",
    "    sample = val_dataset[i]\n",
    "    \n",
    "    encoder_hidden = sample['encoder_hidden_states'].unsqueeze(0).to(device)\n",
    "    encoder_mask = sample['encoder_attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    generated = generate_answer(lightning_model.model, encoder_hidden, encoder_mask, tokenizer)\n",
    "    ground_truth = sample['answer_text']\n",
    "    \n",
    "    em_scores.append(exact_match_score(generated, ground_truth))\n",
    "    f1_scores.append(f1_score(generated, ground_truth))\n",
    "\n",
    "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "avg_em = np.mean(em_scores) * 100\n",
    "avg_f1 = np.mean(f1_scores) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–û–ú –ù–ê–ë–û–†–ï\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Exact Match (EM): {avg_em:.2f}%\")\n",
    "print(f\"F1 Score:         {avg_f1:.2f}%\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
