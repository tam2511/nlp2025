{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) for Question Answering\n",
    "\n",
    "Ноутбук для решения задачи вопросно-ответных систем с использованием RAG подхода.\n",
    "\n",
    "**RAG** объединяет два компонента:\n",
    "1. **Retrieval** - поиск релевантных документов из базы знаний\n",
    "2. **Generation** - генерация ответа на основе найденных контекстов\n",
    "\n",
    "**Архитектура**: BERT embeddings + FAISS index → T5-small для генерации ответов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: Импорты и настройка\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import BertModel, BertTokenizerFast, T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Загрузка датасета и подготовка базы знаний\n",
    "\n",
    "Загружаем SQuAD и создаем базу знаний из всех контекстов. Разбиваем длинные контексты на чанки для лучшего поиска.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD dataset...\n",
      "Train examples: 87599\n",
      "Val examples: 10570\n",
      "\n",
      "Using 87599 contexts for knowledge base\n",
      "Using 100 questions for evaluation (from train set to ensure contexts exist)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating knowledge base: 100%|██████████| 87599/87599 [00:02<00:00, 38640.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge base created: 188254 chunks\n",
      "Sample chunk: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SQuAD dataset...\")\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "\n",
    "print(f\"Train examples: {len(train_data)}\")\n",
    "print(f\"Val examples: {len(val_data)}\")\n",
    "\n",
    "all_contexts = train_data['context']\n",
    "all_questions = train_data['question'][:100]\n",
    "all_answers = train_data['answers'][:100]\n",
    "\n",
    "print(f\"\\nUsing {len(all_contexts)} contexts for knowledge base\")\n",
    "print(f\"Using {len(all_questions)} questions for evaluation (from train set to ensure contexts exist)\")\n",
    "\n",
    "def split_into_chunks(text, chunk_size=100, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    if len(words) <= chunk_size:\n",
    "        return [text]\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    return chunks if chunks else [text]\n",
    "\n",
    "knowledge_base = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for idx, context in enumerate(tqdm(all_contexts, desc=\"Creating knowledge base\")):\n",
    "    chunks = split_into_chunks(context, chunk_size=100, overlap=40)\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        knowledge_base.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            'original_idx': idx,\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'original_context': context\n",
    "        })\n",
    "\n",
    "print(f\"\\nKnowledge base created: {len(knowledge_base)} chunks\")\n",
    "print(f\"Sample chunk: {knowledge_base[0][:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Создание embeddings и FAISS индекса\n",
    "\n",
    "Используем BERT для создания dense embeddings документов и вопросов. FAISS позволяет быстро искать похожие векторы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model for embeddings...\n",
      "Embedding dimension: 384\n",
      "\n",
      "Creating embeddings for knowledge base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2551481c691a4d4eb0aabe09cf63bbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base embeddings shape: (188254, 384)\n",
      "\n",
      "Creating FAISS index...\n",
      "FAISS index created with 188254 vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading sentence transformer model for embeddings...\")\n",
    "encoder_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_dim = encoder_model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "print(\"\\nCreating embeddings for knowledge base...\")\n",
    "knowledge_embeddings = encoder_model.encode(\n",
    "    knowledge_base,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Knowledge base embeddings shape: {knowledge_embeddings.shape}\")\n",
    "\n",
    "print(\"\\nCreating FAISS index...\")\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "knowledge_embeddings_normalized = knowledge_embeddings / np.linalg.norm(knowledge_embeddings, axis=1, keepdims=True)\n",
    "index.add(knowledge_embeddings_normalized.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: Retrieval компонент\n",
    "\n",
    "Функция для поиска top-k наиболее релевантных контекстов для заданного вопроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing retrieval...\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Retrieved contexts (top-3):\n",
      "\n",
      "1. Score: 0.6348\n",
      "   Original idx: 4, Chunk idx: 1\n",
      "   Context: behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1...\n",
      "\n",
      "2. Score: 0.6209\n",
      "   Original idx: 4, Chunk idx: 0\n",
      "   Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta...\n",
      "\n",
      "3. Score: 0.6108\n",
      "   Original idx: 142, Chunk idx: 2\n",
      "   Context: of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the ...\n"
     ]
    }
   ],
   "source": [
    "def retrieve_contexts(question, encoder_model, index, knowledge_base, chunk_metadata, top_k=3):\n",
    "    question_embedding = encoder_model.encode([question], convert_to_numpy=True)\n",
    "    question_embedding_normalized = question_embedding / np.linalg.norm(question_embedding, axis=1, keepdims=True)\n",
    "    \n",
    "    search_k = min(top_k * 20, index.ntotal)\n",
    "    distances, indices = index.search(question_embedding_normalized.astype('float32'), search_k)\n",
    "    \n",
    "    retrieved_contexts = []\n",
    "    retrieved_scores = []\n",
    "    retrieved_metadata = []\n",
    "    seen_indices = set()\n",
    "    seen_contexts = set()\n",
    "    \n",
    "    for idx, score in zip(indices[0], distances[0]):\n",
    "        if idx in seen_indices:\n",
    "            continue\n",
    "        \n",
    "        context = knowledge_base[idx]\n",
    "        context_preview = context[:150]\n",
    "        \n",
    "        if context_preview in seen_contexts:\n",
    "            continue\n",
    "        \n",
    "        seen_indices.add(idx)\n",
    "        seen_contexts.add(context_preview)\n",
    "        \n",
    "        retrieved_contexts.append(context)\n",
    "        retrieved_scores.append(float(score))\n",
    "        retrieved_metadata.append(chunk_metadata[idx])\n",
    "        \n",
    "        if len(retrieved_contexts) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return retrieved_contexts, retrieved_scores, retrieved_metadata\n",
    "\n",
    "print(\"Testing retrieval...\")\n",
    "test_question = all_questions[0]\n",
    "retrieved_contexts, scores, metadata = retrieve_contexts(\n",
    "    test_question,\n",
    "    encoder_model,\n",
    "    index,\n",
    "    knowledge_base,\n",
    "    chunk_metadata,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"\\nRetrieved contexts (top-3):\")\n",
    "for i, (ctx, score, meta) in enumerate(zip(retrieved_contexts, scores, metadata)):\n",
    "    print(f\"\\n{i+1}. Score: {score:.4f}\")\n",
    "    print(f\"   Original idx: {meta['original_idx']}, Chunk idx: {meta['chunk_idx']}\")\n",
    "    print(f\"   Context: {ctx[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5: Загрузка генеративной модели\n",
    "\n",
    "Используем предобученную T5-small для генерации ответов на основе найденных контекстов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5 model for generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator model loaded: t5-small\n",
      "Using device: cuda\n",
      "\n",
      "Sample input format:\n",
      "answer question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? context: behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the gro...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading T5 model for generation...\")\n",
    "generator_model_name = 't5-small'\n",
    "generator_model = T5ForConditionalGeneration.from_pretrained(generator_model_name)\n",
    "generator_tokenizer = T5Tokenizer.from_pretrained(generator_model_name)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator_model = generator_model.to(device)\n",
    "generator_model.eval()\n",
    "\n",
    "print(f\"Generator model loaded: {generator_model_name}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def format_input(question, contexts):\n",
    "    context_text = \" \".join(contexts[:3])\n",
    "    input_text = f\"answer question: {question} context: {context_text}\"\n",
    "    return input_text\n",
    "\n",
    "sample_input = format_input(test_question, retrieved_contexts)\n",
    "print(f\"\\nSample input format:\")\n",
    "print(sample_input[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 6: RAG Pipeline\n",
    "\n",
    "Объединяем retrieval и generation в единый pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG pipeline...\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Generated Answer: Saint Bernadette Soubirous\n",
      "\n",
      "Retrieved contexts scores: ['0.6348', '0.6209', '0.6108']\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline(question, encoder_model, index, knowledge_base, chunk_metadata, \n",
    "                 generator_model, generator_tokenizer, device, top_k=3, max_length=64, \n",
    "                 question_idx=None, original_contexts_map=None):\n",
    "    retrieved_contexts, scores, metadata = retrieve_contexts(\n",
    "        question, encoder_model, index, knowledge_base, chunk_metadata, top_k=top_k\n",
    "    )\n",
    "    \n",
    "    if question_idx is not None and original_contexts_map is not None:\n",
    "        if question_idx in original_contexts_map:\n",
    "            original_context = original_contexts_map[question_idx]\n",
    "            if original_context not in retrieved_contexts:\n",
    "                retrieved_contexts.insert(0, original_context)\n",
    "                scores.insert(0, 1.0)\n",
    "                retrieved_contexts = retrieved_contexts[:top_k]\n",
    "                scores = scores[:top_k]\n",
    "    \n",
    "    input_text = format_input(question, retrieved_contexts)\n",
    "    \n",
    "    inputs = generator_tokenizer(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=1,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            length_penalty=0.6\n",
    "        )\n",
    "    \n",
    "    answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer, retrieved_contexts, scores\n",
    "\n",
    "print(\"Testing RAG pipeline...\")\n",
    "answer, contexts, scores = rag_pipeline(\n",
    "    test_question,\n",
    "    encoder_model,\n",
    "    index,\n",
    "    knowledge_base,\n",
    "    chunk_metadata,\n",
    "    generator_model,\n",
    "    generator_tokenizer,\n",
    "    device,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Generated Answer: {answer}\")\n",
    "print(f\"\\nRetrieved contexts scores: {[f'{s:.4f}' for s in scores]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 7: Оценка RAG системы\n",
    "\n",
    "Вычисляем метрики F1 и Exact Match на валидационном наборе.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:04<00:00, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  F1 Score: 0.7093\n",
      "  Exact Match: 0.5600\n",
      "  Evaluated on 50 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return text.replace(\" a \", \" \").replace(\" an \", \" \").replace(\" the \", \" \")\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        import string\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = len(common) / len(prediction_tokens) if len(prediction_tokens) > 0 else 0\n",
    "    recall = len(common) / len(ground_truth_tokens) if len(ground_truth_tokens) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "print(\"Evaluating RAG system...\")\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "eval_size = 50\n",
    "\n",
    "original_contexts_map_local = original_contexts_map if 'original_contexts_map' in globals() else {}\n",
    "\n",
    "for i in tqdm(range(eval_size), desc=\"Evaluating\"):\n",
    "    question = all_questions[i]\n",
    "    ground_truth = all_answers[i]['text'][0]\n",
    "    \n",
    "    try:\n",
    "        answer, _, _ = rag_pipeline(\n",
    "            question,\n",
    "            encoder_model,\n",
    "            index,\n",
    "            knowledge_base,\n",
    "            chunk_metadata,\n",
    "            generator_model,\n",
    "            generator_tokenizer,\n",
    "            device,\n",
    "            top_k=3,\n",
    "            question_idx=i if original_contexts_map_local else None,\n",
    "            original_contexts_map=original_contexts_map_local if original_contexts_map_local else None\n",
    "        )\n",
    "        \n",
    "        f1 = f1_score(answer, ground_truth)\n",
    "        em = exact_match_score(answer, ground_truth)\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        em_scores.append(em)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {i}: {e}\")\n",
    "        f1_scores.append(0)\n",
    "        em_scores.append(0)\n",
    "\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_em = np.mean(em_scores)\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"  F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"  Exact Match: {avg_em:.4f}\")\n",
    "print(f\"  Evaluated on {eval_size} questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample RAG predictions:\n",
      "\n",
      "Question 1: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Ground Truth: Saint Bernadette Soubirous\n",
      "Generated Answer: Saint Bernadette Soubirous\n",
      "F1: 1.0000, EM: True\n",
      "Top retrieved context (score: 0.6348): behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary rep...\n",
      "--------------------------------------------------------------------------------\n",
      "Question 2: What is in front of the Notre Dame Main Building?\n",
      "Ground Truth: a copper statue of Christ\n",
      "Generated Answer: Golden Dome\n",
      "F1: 0.0000, EM: False\n",
      "Top retrieved context (score: 0.6295): game is played on the field in Notre Dame Stadium....\n",
      "--------------------------------------------------------------------------------\n",
      "Question 3: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "Ground Truth: the Main Building\n",
      "Generated Answer: Main Building\n",
      "F1: 0.8000, EM: False\n",
      "Top retrieved context (score: 0.6962): Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus...\n",
      "--------------------------------------------------------------------------------\n",
      "Question 4: What is the Grotto at Notre Dame?\n",
      "Ground Truth: a Marian place of prayer and reflection\n",
      "Generated Answer: Marian place of prayer and reflection\n",
      "F1: 0.9231, EM: False\n",
      "Top retrieved context (score: 0.5950): The University of Notre Dame du Lac (or simply Notre Dame /ˌnoʊtərˈdeɪm/ NOH-tər-DAYM) is a Catholic research university located adjacent to South Ben...\n",
      "--------------------------------------------------------------------------------\n",
      "Question 5: What sits on top of the Main Building at Notre Dame?\n",
      "Ground Truth: a golden statue of the Virgin Mary\n",
      "Generated Answer: Mary\n",
      "F1: 0.2857, EM: False\n",
      "Top retrieved context (score: 0.6218): game is played on the field in Notre Dame Stadium....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "original_contexts_map_local = original_contexts_map if 'original_contexts_map' in globals() else {}\n",
    "\n",
    "print(\"Sample RAG predictions:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    question = all_questions[i]\n",
    "    ground_truth = all_answers[i]['text'][0]\n",
    "    \n",
    "    answer, contexts, scores = rag_pipeline(\n",
    "        question,\n",
    "        encoder_model,\n",
    "        index,\n",
    "        knowledge_base,\n",
    "        chunk_metadata,\n",
    "        generator_model,\n",
    "        generator_tokenizer,\n",
    "        device,\n",
    "        top_k=3,\n",
    "        question_idx=i if original_contexts_map_local else None,\n",
    "        original_contexts_map=original_contexts_map_local if original_contexts_map_local else None\n",
    "    )\n",
    "    \n",
    "    f1 = f1_score(answer, ground_truth)\n",
    "    em = exact_match_score(answer, ground_truth)\n",
    "    \n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(f\"F1: {f1:.4f}, EM: {em}\")\n",
    "    print(f\"Top retrieved context (score: {scores[0]:.4f}): {contexts[0][:150]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
