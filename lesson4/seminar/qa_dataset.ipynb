{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 1: –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ SQuAD\n",
    "\n",
    "SQuAD (Stanford Question Answering Dataset) - —ç—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∑–∞–¥–∞—á–∏ —á—Ç–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –≥–¥–µ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.\n",
    "\n",
    "–ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç:\n",
    "- **context**: –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Ç–µ–∫—Å—Ç–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç\n",
    "- **question**: –≤–æ–ø—Ä–æ—Å\n",
    "- **answers**: —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ (start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD dataset...\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "\n",
      "Train set size: 87599\n",
      "Validation set size: 10570\n",
      "\n",
      "Sample example:\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta...\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answers: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "\n",
      "Answer text: Saint Bernadette Soubirous\n",
      "Answer start: 515\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SQuAD dataset...\")\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nTrain set size: {len(dataset['train'])}\")\n",
    "print(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "\n",
    "print(\"\\nSample example:\")\n",
    "example = dataset['train'][0]\n",
    "print(f\"Context: {example['context'][:200]}...\")\n",
    "print(f\"\\nQuestion: {example['question']}\")\n",
    "print(f\"\\nAnswers: {example['answers']}\")\n",
    "print(f\"\\nAnswer text: {example['answers']['text'][0]}\")\n",
    "print(f\"Answer start: {example['answers']['answer_start'][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å\n",
    "\n",
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –ø–∞–º—è—Ç—å –¥–ª—è —É–¥–æ–±–Ω–æ–π —Ä–∞–±–æ—Ç—ã.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87599 training examples\n",
      "Loaded 10570 validation examples\n",
      "\n",
      "Example from training set:\n",
      "Context length: 695 characters\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer text: Saint Bernadette Soubirous\n",
      "Answer start position: 515\n",
      "Answer end position: 541\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "\n",
    "train_contexts = train_data['context']\n",
    "train_questions = train_data['question']\n",
    "train_answers = train_data['answers']\n",
    "\n",
    "val_contexts = val_data['context']\n",
    "val_questions = val_data['question']\n",
    "val_answers = val_data['answers']\n",
    "\n",
    "print(f\"Loaded {len(train_contexts)} training examples\")\n",
    "print(f\"Loaded {len(val_contexts)} validation examples\")\n",
    "\n",
    "print(f\"\\nExample from training set:\")\n",
    "print(f\"Context length: {len(train_contexts[0])} characters\")\n",
    "print(f\"Question: {train_questions[0]}\")\n",
    "print(f\"Answer text: {train_answers[0]['text'][0]}\")\n",
    "print(f\"Answer start position: {train_answers[0]['answer_start'][0]}\")\n",
    "print(f\"Answer end position: {train_answers[0]['answer_start'][0] + len(train_answers[0]['text'][0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ PyTorch Dataset –æ–±–µ—Ä—Ç–∫–∏\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å `QADataset`, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Å–ª–µ–¥—É–µ—Ç—Å—è –æ—Ç `torch.utils.data.Dataset` –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å QA –¥–∞–Ω–Ω—ã–º–∏.\n",
    "\n",
    "–ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å:\n",
    "- context: —Ç–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "- question: –≤–æ–ø—Ä–æ—Å\n",
    "- answer_text: —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞\n",
    "- answer_start: –Ω–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ\n",
    "- answer_end: –∫–æ–Ω–µ—á–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \n",
    "    def __init__(self, contexts, questions, answers):\n",
    "        self.contexts = contexts\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        \n",
    "        self.answer_texts = []\n",
    "        self.answer_starts = []\n",
    "        self.answer_ends = []\n",
    "        \n",
    "        for answer_dict in answers:\n",
    "            answer_text = answer_dict['text'][0]\n",
    "            answer_start = answer_dict['answer_start'][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "            \n",
    "            self.answer_texts.append(answer_text)\n",
    "            self.answer_starts.append(answer_start)\n",
    "            self.answer_ends.append(answer_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'context': self.contexts[idx],\n",
    "            'question': self.questions[idx],\n",
    "            'answer_text': self.answer_texts[idx],\n",
    "            'answer_start': self.answer_starts[idx],\n",
    "            'answer_end': self.answer_ends[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 5: –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º train –∏ validation –¥–∞—Ç–∞—Å–µ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—à—É –æ–±–µ—Ä—Ç–∫—É.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 87599\n",
      "Validation dataset size: 10570\n",
      "\n",
      "Sample from dataset:\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front o...\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer text: Saint Bernadette Soubirous\n",
      "Answer start: 515\n",
      "Answer end: 541\n",
      "\n",
      "Verification - context slice at answer position: 'Saint Bernadette Soubirous'\n",
      "Matches answer text: True\n"
     ]
    }
   ],
   "source": [
    "train_dataset = QADataset(train_contexts, train_questions, train_answers)\n",
    "val_dataset = QADataset(val_contexts, val_questions, val_answers)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample from dataset:\")\n",
    "print(f\"Context: {sample['context'][:150]}...\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer text: {sample['answer_text']}\")\n",
    "print(f\"Answer start: {sample['answer_start']}\")\n",
    "print(f\"Answer end: {sample['answer_end']}\")\n",
    "\n",
    "context_slice = sample['context'][sample['answer_start']:sample['answer_end']]\n",
    "print(f\"\\nVerification - context slice at answer position: '{context_slice}'\")\n",
    "print(f\"Matches answer text: {context_slice == sample['answer_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataLoader\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader...\n",
      "\n",
      "Batch 1:\n",
      "  Batch size: 4\n",
      "  Context lengths: [687, 905, 849, 253]\n",
      "  Questions: ['The royal courts sponsored both Buddhism and what?', 'What was the controversial domestic surveillance operation in this era?', 'How can religious beliefs contribute to a person remaining in pain?', 'Aside from the koofiyad, what do Somali men wear on their head?']\n",
      "  Answer texts: ['Saivism', 'COINTELPRO', 'prevent the individual from seeking help', 'turban']\n",
      "\n",
      "Batch 2:\n",
      "  Batch size: 4\n",
      "  Context lengths: [526, 867, 817, 768]\n",
      "  Questions: ['Where is the Gold State Coach kept?', 'How is DNA grouping superior?', 'When was Montini absent from the conclave?', 'When were women first admitted to Northwestern?']\n",
      "  Answer texts: ['the Royal Mews', 'genetic code itself is used', '1958', '1869']\n",
      "\n",
      "DataLoader —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Testing DataLoader...\")\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"\\nBatch {i+1}:\")\n",
    "    print(f\"  Batch size: {len(batch['context'])}\")\n",
    "    print(f\"  Context lengths: {[len(ctx) for ctx in batch['context']]}\")\n",
    "    print(f\"  Questions: {batch['question']}\")\n",
    "    print(f\"  Answer texts: {batch['answer_text']}\")\n",
    "    if i >= 1:\n",
    "        break\n",
    "\n",
    "print(\"\\nDataLoader —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 7: –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BERT\n",
    "\n",
    "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å BERT –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã [CLS] –∏ [SEP].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: bert-base-uncased\n",
      "Vocab size: 30522\n",
      "\n",
      "Sample encoding:\n",
      "  Input IDs shape: torch.Size([1, 9])\n",
      "  Input IDs: tensor([[ 101, 7592, 1010, 2023, 2003, 1037, 3231, 1012,  102]])\n",
      "  Decoded: [CLS] hello, this is a test. [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer loaded: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "sample_text = \"Hello, this is a test.\"\n",
    "encoded = tokenizer(sample_text, return_tensors='pt', padding=True, truncation=True)\n",
    "print(f\"\\nSample encoding:\")\n",
    "print(f\"  Input IDs shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"  Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"  Decoded: {tokenizer.decode(encoded['input_ids'][0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 8: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Dataset —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π\n",
    "\n",
    "–ù—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å Dataset –∫–ª–∞—Å—Å, —á—Ç–æ–±—ã –æ–Ω —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞–ª –ø–æ–∑–∏—Ü–∏–∏ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è QA –∑–∞–¥–∞—á–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATokenizedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, contexts, questions, answers, tokenizer, max_length=384):\n",
    "        self.contexts = contexts\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.answer_texts = []\n",
    "        self.answer_starts = []\n",
    "        self.answer_ends = []\n",
    "        \n",
    "        for answer_dict in answers:\n",
    "            answer_text = answer_dict['text'][0]\n",
    "            answer_start = answer_dict['answer_start'][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "            \n",
    "            self.answer_texts.append(answer_text)\n",
    "            self.answer_starts.append(answer_start)\n",
    "            self.answer_ends.append(answer_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        answer_start_char = self.answer_starts[idx]\n",
    "        answer_end_char = self.answer_ends[idx]\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        offset_mapping = encoded['offset_mapping'].squeeze(0)\n",
    "        \n",
    "        sep_token_id = self.tokenizer.sep_token_id\n",
    "        sep_positions = (input_ids == sep_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(sep_positions) > 0:\n",
    "            context_start_token = sep_positions[0].item() + 1\n",
    "        else:\n",
    "            context_start_token = 1\n",
    "        \n",
    "        context_start_char = None\n",
    "        for i in range(context_start_token, len(offset_mapping)):\n",
    "            start_char, end_char = offset_mapping[i]\n",
    "            if start_char != 0 or end_char != 0:\n",
    "                context_start_char = start_char\n",
    "                break\n",
    "        \n",
    "        if context_start_char is None:\n",
    "            context_start_char = 0\n",
    "        \n",
    "        start_pos = 0\n",
    "        end_pos = 0\n",
    "        \n",
    "        for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "            if start_char == 0 and end_char == 0:\n",
    "                continue\n",
    "            if i < context_start_token:\n",
    "                continue\n",
    "            \n",
    "            if start_char >= context_start_char:\n",
    "                char_pos_in_context = start_char - context_start_char\n",
    "                \n",
    "                if start_pos == 0 and char_pos_in_context <= answer_start_char < end_char - context_start_char:\n",
    "                    start_pos = i\n",
    "                if char_pos_in_context < answer_end_char <= end_char - context_start_char:\n",
    "                    end_pos = i\n",
    "        \n",
    "        if start_pos == 0 or end_pos == 0 or start_pos > end_pos:\n",
    "            start_pos = context_start_token\n",
    "            end_pos = min(context_start_token + 1, len(input_ids) - 1)\n",
    "        \n",
    "        start_pos = min(start_pos, len(input_ids) - 1)\n",
    "        end_pos = min(end_pos, len(input_ids) - 1)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'start_positions': torch.tensor(start_pos, dtype=torch.long),\n",
    "            'end_positions': torch.tensor(end_pos, dtype=torch.long),\n",
    "            'answer_text': self.answer_texts[idx],\n",
    "            'context': context,\n",
    "            'question': question\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 9: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π. –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokenized dataset size: 10000\n",
      "Val tokenized dataset size: 1000\n",
      "\n",
      "Sample from tokenized dataset:\n",
      "Input IDs shape: torch.Size([384])\n",
      "Start position: 130\n",
      "End position: 137\n",
      "Answer text: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "train_size = 10000\n",
    "val_size = 1000\n",
    "\n",
    "train_tokenized_dataset = QATokenizedDataset(\n",
    "    train_contexts[:train_size],\n",
    "    train_questions[:train_size],\n",
    "    train_answers[:train_size],\n",
    "    tokenizer,\n",
    "    max_length=384\n",
    ")\n",
    "\n",
    "val_tokenized_dataset = QATokenizedDataset(\n",
    "    val_contexts[:val_size],\n",
    "    val_questions[:val_size],\n",
    "    val_answers[:val_size],\n",
    "    tokenizer,\n",
    "    max_length=384\n",
    ")\n",
    "\n",
    "print(f\"Train tokenized dataset size: {len(train_tokenized_dataset)}\")\n",
    "print(f\"Val tokenized dataset size: {len(val_tokenized_dataset)}\")\n",
    "\n",
    "sample = train_tokenized_dataset[0]\n",
    "print(f\"\\nSample from tokenized dataset:\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Start position: {sample['start_positions']}\")\n",
    "print(f\"End position: {sample['end_positions']}\")\n",
    "print(f\"Answer text: {sample['answer_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 10: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ QA –Ω–∞ BERT\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç BERT –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –¥–≤–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è start –∏ end –ø–æ–∑–∏—Ü–∏–π –æ—Ç–≤–µ—Ç–∞.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base QA model class created\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class QABertModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(QABertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.qa_outputs = nn.Linear(self.hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "\n",
    "print(\"Base QA model class created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 11: Lightning –º–æ–¥—É–ª—å –¥–ª—è QA\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º PyTorch Lightning –º–æ–¥—É–ª—å —Å training/validation —à–∞–≥–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ F1 –∏ Exact Match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning module created\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return text.replace(\" a \", \" \").replace(\" an \", \" \").replace(\" the \", \" \")\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        import string\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = len(common) / len(prediction_tokens) if len(prediction_tokens) > 0 else 0\n",
    "    recall = len(common) / len(ground_truth_tokens) if len(ground_truth_tokens) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "class QALightningModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', lr=2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = QABertModel(model_name)\n",
    "        self.lr = lr\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        start_positions = batch['start_positions']\n",
    "        end_positions = batch['end_positions']\n",
    "        \n",
    "        start_logits, end_logits = self(input_ids, attention_mask)\n",
    "        \n",
    "        seq_length = start_logits.size(1)\n",
    "        ignored_index = seq_length\n",
    "        start_positions = start_positions.clamp(0, seq_length - 1)\n",
    "        end_positions = end_positions.clamp(0, seq_length - 1)\n",
    "        \n",
    "        \n",
    "        start_loss = self.loss(start_logits, start_positions)\n",
    "        end_loss = self.loss(end_logits, end_positions)\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        start_positions = batch['start_positions']\n",
    "        end_positions = batch['end_positions']\n",
    "        answer_texts = batch['answer_text']\n",
    "        \n",
    "        start_logits, end_logits = self(input_ids, attention_mask)\n",
    "        \n",
    "        seq_length = start_logits.size(1)\n",
    "        ignored_index = seq_length\n",
    "        start_positions = start_positions.clamp(0, seq_length - 1)\n",
    "        end_positions = end_positions.clamp(0, seq_length - 1)\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "        start_loss = loss_fct(start_logits, start_positions)\n",
    "        end_loss = loss_fct(end_logits, end_positions)\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        start_preds = start_logits.argmax(dim=1)\n",
    "        end_preds = end_logits.argmax(dim=1)\n",
    "        \n",
    "        batch_f1_scores = []\n",
    "        batch_em_scores = []\n",
    "        \n",
    "        for i in range(len(answer_texts)):\n",
    "            start_idx = start_preds[i].item()\n",
    "            end_idx = end_preds[i].item()\n",
    "            \n",
    "            if start_idx > end_idx:\n",
    "                predicted_text = \"\"\n",
    "            else:\n",
    "                token_ids = input_ids[i][start_idx:end_idx+1]\n",
    "                predicted_text = self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "            \n",
    "            ground_truth = answer_texts[i]\n",
    "            \n",
    "            f1 = f1_score(predicted_text, ground_truth)\n",
    "            em = exact_match_score(predicted_text, ground_truth)\n",
    "            \n",
    "            batch_f1_scores.append(f1)\n",
    "            batch_em_scores.append(em)\n",
    "        \n",
    "        avg_f1 = np.mean(batch_f1_scores)\n",
    "        avg_em = np.mean(batch_em_scores)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', avg_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_em', avg_em, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "print(\"Lightning module created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 12: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å Lightning\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º DataLoaders –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch Lightning Trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1250\n",
      "Val batches: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA L40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | QABertModel | 109 M  | train\n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=77` in the `DataLoader` to improve performance.\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=77` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3410723f1fc4062b3963d8c9c325d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_tokenized_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_tokenized_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "lightning_model = QALightningModule(model_name=model_name, lr=2e-5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=50,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.fit(lightning_model, train_loader, val_loader)\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football...\n",
      "Predicted: american football conference ( afc ) champion denver broncos\n",
      "Ground truth: Denver Broncos\n",
      "F1: 0.4444\n",
      "EM: False\n",
      "\n",
      "Question: Which NFL team represented the NFC at Super Bowl 50?\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football...\n",
      "Predicted: \n",
      "Ground truth: Carolina Panthers\n",
      "F1: 0.0000\n",
      "EM: False\n",
      "\n",
      "Question: Where did Super Bowl 50 take place?\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football...\n",
      "Predicted: levi ' s stadium in the san francisco bay area at santa clara, california\n",
      "Ground truth: Santa Clara, California\n",
      "F1: 0.4000\n",
      "EM: False\n"
     ]
    }
   ],
   "source": [
    "lightning_model.eval()\n",
    "sample_batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = sample_batch['input_ids'][:3]\n",
    "    attention_mask = sample_batch['attention_mask'][:3]\n",
    "    \n",
    "    start_logits, end_logits = lightning_model(input_ids, attention_mask)\n",
    "    start_preds = start_logits.argmax(dim=1)\n",
    "    end_preds = end_logits.argmax(dim=1)\n",
    "    \n",
    "    for i in range(3):\n",
    "        start_idx = start_preds[i].item()\n",
    "        end_idx = end_preds[i].item()\n",
    "        \n",
    "        if start_idx > end_idx:\n",
    "            predicted_text = \"\"\n",
    "        else:\n",
    "            token_ids = input_ids[i][start_idx:end_idx+1]\n",
    "            predicted_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nQuestion: {sample_batch['question'][i]}\")\n",
    "        print(f\"Context: {sample_batch['context'][i][:150]}...\")\n",
    "        print(f\"Predicted: {predicted_text}\")\n",
    "        print(f\"Ground truth: {sample_batch['answer_text'][i]}\")\n",
    "        print(f\"F1: {f1_score(predicted_text, sample_batch['answer_text'][i]):.4f}\")\n",
    "        print(f\"EM: {exact_match_score(predicted_text, sample_batch['answer_text'][i])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
