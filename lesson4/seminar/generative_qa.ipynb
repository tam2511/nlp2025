{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Question Answering with BERT + Decoder\n",
    "\n",
    "ÐÐ¾ÑƒÑ‚Ð±ÑƒÐº Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ½Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚Ð½Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ BERT encoder Ð¸ Transformer decoder.\n",
    "\n",
    "**Ð¦ÐµÐ»ÑŒ**: Ð¾Ð±ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ BERT Ð´Ð»Ñ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°, Ð¸ decoder Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.\n",
    "\n",
    "**ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°**: BERT-base-uncased (encoder) + Transformer Decoder â†’ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 1: Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 2: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°\n",
    "\n",
    "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ SQuAD Ð´Ð°Ñ‚Ð°ÑÐµÑ‚. Ð”Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½ÑƒÐ¶ÐµÐ½ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð¸ Ð´Ð»Ñ encoder (BERT) Ð¸ Ð´Ð»Ñ decoder (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ñ‚ Ð¶Ðµ BERT Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD dataset...\n",
      "Train examples: 87599\n",
      "Val examples: 10570\n",
      "\n",
      "Tokenizer loaded: bert-base-uncased\n",
      "Vocab size: 30522\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "\n",
      "Sample:\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SQuAD dataset...\")\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "\n",
    "# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚\n",
    "train_contexts = train_data['context']\n",
    "train_questions = train_data['question']\n",
    "train_answers = train_data['answers']\n",
    "\n",
    "val_contexts = val_data['context']\n",
    "val_questions = val_data['question']\n",
    "val_answers = val_data['answers']\n",
    "\n",
    "print(f\"Train examples: {len(train_contexts)}\")\n",
    "print(f\"Val examples: {len(val_contexts)}\")\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\nTokenizer loaded: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "sample_question = train_questions[0]\n",
    "sample_context = train_contexts[0]\n",
    "sample_answer = train_answers[0]['text'][0]\n",
    "\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Answer: {sample_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 3: Dataset Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "\n",
    "Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Dataset ÐºÐ»Ð°ÑÑ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚:\n",
    "- Input: question + context (Ð´Ð»Ñ encoder)\n",
    "- Target: answer (Ð´Ð»Ñ decoder, Ñ teacher forcing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 87599\n",
      "Val dataset size: 10570\n",
      "\n",
      "Sample from dataset:\n",
      "Encoder input IDs shape: torch.Size([384])\n",
      "Decoder input IDs shape: torch.Size([64])\n",
      "Decoder target IDs shape: torch.Size([64])\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "class GenerativeQADataset(Dataset):\n",
    "    \n",
    "    def __init__(self, contexts, questions, answers, tokenizer, max_encoder_length=384, max_decoder_length=64):\n",
    "        self.contexts = contexts\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_encoder_length = max_encoder_length\n",
    "        self.max_decoder_length = max_decoder_length\n",
    "        \n",
    "        self.answer_texts = []\n",
    "        for answer_dict in answers:\n",
    "            answer_text = answer_dict['text'][0]\n",
    "            self.answer_texts.append(answer_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        answer = self.answer_texts[idx]\n",
    "        \n",
    "        # ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð°Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ encoder input: question + context\n",
    "        # tokenizer Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ [CLS] question [SEP] context [SEP]\n",
    "        encoder_encoded = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            max_length=self.max_encoder_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð‘Ð•Ð— ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²\n",
    "        answer_tokens = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "        \n",
    "        # ÐžÐ±Ñ€ÐµÐ·Ð°ÐµÐ¼ ÐµÑÐ»Ð¸ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¹ (Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑÑ‚Ð¾ Ð´Ð»Ñ [CLS] Ð¸ [SEP])\n",
    "        max_answer_len = self.max_decoder_length - 2\n",
    "        answer_tokens = answer_tokens[:max_answer_len]\n",
    "        \n",
    "        # Teacher forcing:\n",
    "        # decoder_input:  [CLS] word1 word2 ... wordN [PAD] [PAD] ...\n",
    "        # decoder_target: word1 word2 ... wordN [SEP] [PAD] [PAD] ...\n",
    "        \n",
    "        decoder_input_ids = [self.tokenizer.cls_token_id] + answer_tokens\n",
    "        decoder_target_ids = answer_tokens + [self.tokenizer.sep_token_id]\n",
    "        \n",
    "        # ÐŸÐ°Ð´Ð´Ð¸Ð½Ð³ Ð´Ð¾ max_decoder_length\n",
    "        pad_len = self.max_decoder_length - len(decoder_input_ids)\n",
    "        decoder_input_ids = decoder_input_ids + [self.tokenizer.pad_token_id] * pad_len\n",
    "        decoder_target_ids = decoder_target_ids + [self.tokenizer.pad_token_id] * pad_len\n",
    "        \n",
    "        # Attention mask: 1 Ð´Ð»Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², 0 Ð´Ð»Ñ padding\n",
    "        decoder_attention_mask = [1] * (len(answer_tokens) + 1) + [0] * pad_len\n",
    "        \n",
    "        return {\n",
    "            'encoder_input_ids': encoder_encoded['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': encoder_encoded['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
    "            'decoder_attention_mask': torch.tensor(decoder_attention_mask, dtype=torch.long),\n",
    "            'decoder_target_ids': torch.tensor(decoder_target_ids, dtype=torch.long),\n",
    "            'answer_text': answer,\n",
    "            'question': question,\n",
    "            'context': context\n",
    "        }\n",
    "\n",
    "train_dataset = GenerativeQADataset(\n",
    "    train_contexts,\n",
    "    train_questions,\n",
    "    train_answers,\n",
    "    tokenizer,\n",
    "    max_encoder_length=384,\n",
    "    max_decoder_length=64\n",
    ")\n",
    "\n",
    "val_dataset = GenerativeQADataset(\n",
    "    val_contexts,\n",
    "    val_questions,\n",
    "    val_answers,\n",
    "    tokenizer,\n",
    "    max_encoder_length=384,\n",
    "    max_decoder_length=64\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample from dataset:\")\n",
    "print(f\"Encoder input IDs shape: {sample['encoder_input_ids'].shape}\")\n",
    "print(f\"Decoder input IDs shape: {sample['decoder_input_ids'].shape}\")\n",
    "print(f\"Decoder target IDs shape: {sample['decoder_target_ids'].shape}\")\n",
    "print(f\"Answer: {sample['answer_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Decoder created\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=768, nhead=12, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        output = tgt\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, tgt_mask, memory_key_padding_mask, tgt_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "print(\"Transformer Decoder created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 5: BERT + Decoder Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "\n",
    "Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: BERT encoder + Transformer decoder + output projection Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT + Decoder model created\n"
     ]
    }
   ],
   "source": [
    "class BERTGenerativeQA(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', vocab_size=30522, d_model=768, nhead=12, \n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = BertModel.from_pretrained(model_name)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.decoder = TransformerDecoder(d_model, nhead, num_decoder_layers, dim_feedforward, dropout)\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask=None):\n",
    "        encoder_outputs = self.encoder(input_ids=encoder_input_ids, attention_mask=encoder_attention_mask)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        decoder_embeds = self.decoder_embedding(decoder_input_ids) * math.sqrt(self.d_model)\n",
    "        decoder_embeds = self.decoder_pos_encoding(decoder_embeds)\n",
    "        \n",
    "        tgt_mask = self.generate_square_subsequent_mask(decoder_input_ids.size(1)).to(decoder_input_ids.device)\n",
    "        \n",
    "        decoder_outputs = self.decoder(\n",
    "            decoder_embeds.transpose(0, 1),\n",
    "            encoder_hidden_states.transpose(0, 1),\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=(encoder_attention_mask == 0),\n",
    "            tgt_key_padding_mask=(decoder_attention_mask == 0) if decoder_attention_mask is not None else None\n",
    "        )\n",
    "        \n",
    "        decoder_outputs = decoder_outputs.transpose(0, 1)\n",
    "        logits = self.output_projection(decoder_outputs)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"BERT + Decoder model created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 6: Lightning Ð¼Ð¾Ð´ÑƒÐ»ÑŒ\n",
    "\n",
    "Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Lightning Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning module created\n"
     ]
    }
   ],
   "source": [
    "class GenerativeQALightningModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', vocab_size=30522, d_model=768, \n",
    "                 nhead=12, num_decoder_layers=3, dim_feedforward=2048, dropout=0.1, \n",
    "                 encoder_lr=2e-5, decoder_lr=1e-4, warmup_epochs=2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = BERTGenerativeQA(\n",
    "            model_name=model_name,\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.encoder_lr = encoder_lr\n",
    "        self.decoder_lr = decoder_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        \n",
    "    def forward(self, encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask=None):\n",
    "        return self.model(encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        encoder_input_ids = batch['encoder_input_ids']\n",
    "        encoder_attention_mask = batch['encoder_attention_mask']\n",
    "        decoder_input_ids = batch['decoder_input_ids']\n",
    "        decoder_attention_mask = batch['decoder_attention_mask']\n",
    "        decoder_target_ids = batch['decoder_target_ids']\n",
    "        \n",
    "        logits = self(encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask)\n",
    "        \n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        targets = decoder_target_ids.view(-1)\n",
    "        \n",
    "        loss = self.criterion(logits, targets)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        encoder_input_ids = batch['encoder_input_ids']\n",
    "        encoder_attention_mask = batch['encoder_attention_mask']\n",
    "        decoder_input_ids = batch['decoder_input_ids']\n",
    "        decoder_attention_mask = batch['decoder_attention_mask']\n",
    "        decoder_target_ids = batch['decoder_target_ids']\n",
    "        answer_texts = batch['answer_text']\n",
    "        \n",
    "        logits = self(encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask)\n",
    "        \n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        targets = decoder_target_ids.view(-1)\n",
    "        \n",
    "        loss = self.criterion(logits, targets)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Ð Ð°Ð·Ð½Ñ‹Ðµ learning rates Ð´Ð»Ñ encoder (BERT) Ð¸ decoder\n",
    "        encoder_params = list(self.model.encoder.parameters())\n",
    "        decoder_params = (\n",
    "            list(self.model.decoder_embedding.parameters()) +\n",
    "            list(self.model.decoder_pos_encoding.parameters()) +\n",
    "            list(self.model.decoder.parameters()) +\n",
    "            list(self.model.output_projection.parameters())\n",
    "        )\n",
    "        \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': encoder_params, 'lr': self.encoder_lr},\n",
    "            {'params': decoder_params, 'lr': self.decoder_lr}\n",
    "        ], betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
    "        \n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = int(total_steps * (self.warmup_epochs / self.trainer.max_epochs)) if self.trainer.max_epochs > 0 else int(total_steps * 0.1)\n",
    "        \n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < warmup_steps:\n",
    "                return float(current_step) / float(max(1, warmup_steps))\n",
    "            return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Lightning module created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 7: ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "\n",
    "Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataLoaders Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 685\n",
      "Val batches: 331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA L40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | BERTGenerativeQA | 180 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "180 M     Trainable params\n",
      "0         Non-trainable params\n",
      "180 M     Total params\n",
      "720.113   Total estimated model params size (MB)\n",
      "51        Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1749d1d78d8746f584891bfce92348a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 47. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "lightning_model = GenerativeQALightningModule(\n",
    "    model_name=model_name,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768,\n",
    "    nhead=12,\n",
    "    num_decoder_layers=3,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    encoder_lr=2e-5,   # ÐœÐµÐ½ÑŒÑˆÐ¸Ð¹ lr Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ BERT\n",
    "    decoder_lr=1e-4,   # Ð‘Ð¾Ð»ÑŒÑˆÐ¸Ð¹ lr Ð´Ð»Ñ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ decoder\n",
    "    warmup_epochs=2\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=50,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=1,\n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.fit(lightning_model, train_loader, val_loader)\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¨Ð°Ð³ 8: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²\n",
    "\n",
    "Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¸Ð· Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð½Ð°Ð±Ð¾Ñ€Ð°.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation with different strategies:\n",
      "\n",
      "\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football...\n",
      "Ground Truth: Denver Broncos\n",
      "Generated (greedy): 'colorado giants'\n",
      "Generated (sampled, temp=0.7): 'colorado giants'\n",
      "\n",
      "Question: Which NFL team represented the NFC at Super Bowl 50?\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football...\n",
      "Ground Truth: Carolina Panthers\n",
      "Generated (greedy): 'colorado giants'\n",
      "Generated (sampled, temp=0.7): 'dc cubs'\n",
      "\n",
      "Question: Where did Super Bowl 50 take place?\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football...\n",
      "Ground Truth: Santa Clara, California\n",
      "Generated (greedy): 'san francisco bay'\n",
      "Generated (sampled, temp=0.7): 'san francisco bay'\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(model, encoder_input_ids, encoder_attention_mask, tokenizer, max_length=64, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    device = encoder_input_ids.device\n",
    "    \n",
    "    decoder_input_ids = torch.tensor([[tokenizer.cls_token_id]], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            decoder_attention_mask = torch.ones_like(decoder_input_ids, device=device)\n",
    "            \n",
    "            logits = model(encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            if top_k > 0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, min(top_k, next_token_logits.size(0)))\n",
    "                next_token_logits_filtered = torch.full_like(next_token_logits, float('-inf'))\n",
    "                next_token_logits_filtered.scatter_(0, top_k_indices, top_k_logits)\n",
    "                probs = F.softmax(next_token_logits_filtered, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "            else:\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer.sep_token_id or next_token_id.item() == tokenizer.pad_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_ids = decoder_input_ids[0].cpu().tolist()\n",
    "    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    if answer.startswith(tokenizer.cls_token):\n",
    "        answer = answer[len(tokenizer.cls_token):].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "lightning_model.eval()\n",
    "device = next(lightning_model.parameters()).device\n",
    "sample_batch = next(iter(val_loader))\n",
    "\n",
    "print(\"Testing generation with different strategies:\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    encoder_input_ids = sample_batch['encoder_input_ids'][i:i+1].to(device)\n",
    "    encoder_attention_mask = sample_batch['encoder_attention_mask'][i:i+1].to(device)\n",
    "    \n",
    "    print(f\"\\nQuestion: {sample_batch['question'][i]}\")\n",
    "    print(f\"Context: {sample_batch['context'][i][:150]}...\")\n",
    "    print(f\"Ground Truth: {sample_batch['answer_text'][i]}\")\n",
    "    \n",
    "    generated_greedy = generate_answer(\n",
    "        lightning_model.model,\n",
    "        encoder_input_ids,\n",
    "        encoder_attention_mask,\n",
    "        tokenizer,\n",
    "        max_length=64,\n",
    "        temperature=1.0,\n",
    "        top_k=0\n",
    "    )\n",
    "    print(f\"Generated (greedy): '{generated_greedy}'\")\n",
    "    \n",
    "    generated_sampled = generate_answer(\n",
    "        lightning_model.model,\n",
    "        encoder_input_ids,\n",
    "        encoder_attention_mask,\n",
    "        tokenizer,\n",
    "        max_length=64,\n",
    "        temperature=0.7,\n",
    "        top_k=50\n",
    "    )\n",
    "    print(f\"Generated (sampled, temp=0.7): '{generated_sampled}'\")\n",
    "    \n",
    "    if not generated_greedy or generated_greedy.strip() == \"\":\n",
    "        print(\"WARNING: Empty answer generated!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
