{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация и классика в NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: Загрузка датасета\n",
    "\n",
    "Начнем с загрузки датасета AG News. AG News — это датасет для классификации новостных статей с 4 классами:\n",
    "- World\n",
    "- Sports\n",
    "- Business\n",
    "- Sci/Tech\n",
    "\n",
    "Датасет содержит приблизительно 120,000 примеров для обучения и 7,600 примеров для тестирования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AG News dataset...\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "\n",
      "Train set size: 120000\n",
      "Test set size: 7600\n",
      "\n",
      "Sample examples:\n",
      "\n",
      "Example 1:\n",
      "  Label: 2 (Business)\n",
      "  Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n",
      "\n",
      "Example 2:\n",
      "  Label: 2 (Business)\n",
      "  Text: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense in...\n",
      "\n",
      "Example 3:\n",
      "  Label: 2 (Business)\n",
      "  Text: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during t...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nTrain set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "\n",
    "print(\"\\nSample examples:\")\n",
    "for i in range(3):\n",
    "    example = dataset['train'][i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Label: {example['label']} ({dataset['train'].features['label'].names[example['label']]})\")\n",
    "    print(f\"  Text: {example['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into memory...\n",
      "\n",
      "Train texts loaded: 120000\n",
      "Train labels loaded: 120000\n",
      "Test texts loaded: 7600\n",
      "Test labels loaded: 7600\n",
      "\n",
      "Label distribution in training set:\n",
      "  World: 30000 (25.0%)\n",
      "  Sports: 30000 (25.0%)\n",
      "  Business: 30000 (25.0%)\n",
      "  Sci/Tech: 30000 (25.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data into memory...\")\n",
    "\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "print(f\"\\nTrain texts loaded: {len(train_texts)}\")\n",
    "print(f\"Train labels loaded: {len(train_labels)}\")\n",
    "print(f\"Test texts loaded: {len(test_texts)}\")\n",
    "print(f\"Test labels loaded: {len(test_labels)}\")\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    label_name = dataset['train'].features['label'].names[label]\n",
    "    print(f\"  {label_name}: {count} ({count/len(train_labels)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics:\n",
      "  Average text length (train): 250.2 characters\n",
      "  Average text length (test): 242.2 characters\n",
      "  Number of classes: 4\n",
      "  Class names: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "✓ Dataset successfully loaded into memory!\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset statistics:\")\n",
    "print(f\"  Average text length (train): {np.mean([len(text) for text in train_texts[:1000]]):.1f} characters\")\n",
    "print(f\"  Average text length (test): {np.mean([len(text) for text in test_texts[:1000]]):.1f} characters\")\n",
    "print(f\"  Number of classes: {len(dataset['train'].features['label'].names)}\")\n",
    "print(f\"  Class names: {dataset['train'].features['label'].names}\")\n",
    "\n",
    "print(\"\\n✓ Dataset successfully loaded into memory!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Токенизация текста\n",
    "\n",
    "Токенизация — это процесс разбиения текста на отдельные единицы (токены). Рассмотрим различные подходы к токенизации, каждый из которых имеет свои преимущества и области применения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level токенизация\n",
    "\n",
    "Самый простой подход — разделение текста по пробелам. Каждое слово становится отдельным токеном. Это базовый метод для классических подходов типа Bag-of-Words и TF-IDF.\n",
    "\n",
    "**Принцип работы:** Текст разбивается по пробелам и знакам препинания, каждое слово приводится к нижнему регистру и нормализуется.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n",
      "\n",
      "Word-level tokens (24 tokens):\n",
      "['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 's', 'dwindling', 'band', 'of', 'ultra', 'cynics']\n",
      "\n",
      "Vocabulary size (first 1000 texts): 7552\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def word_level_tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "sample_text = train_texts[0]\n",
    "word_tokens = word_level_tokenize(sample_text)\n",
    "\n",
    "print(f\"Original text: {sample_text[:150]}...\")\n",
    "print(f\"\\nWord-level tokens ({len(word_tokens)} tokens):\")\n",
    "print(word_tokens[:20])\n",
    "print(f\"\\nVocabulary size (first 1000 texts): {len(set([token for text in train_texts[:1000] for token in word_level_tokenize(text)]))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level токенизация\n",
    "\n",
    "Токенизация на уровне символов разбивает текст на отдельные символы. Можно также использовать char n-grams (последовательности из n символов). Такой подход устойчив к опечаткам и смешанным алфавитам, но создает очень длинные последовательности.\n",
    "\n",
    "**Принцип работы:** Каждый символ становится отдельным токеном. Можно также создавать n-граммы символов для захвата локальных паттернов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\b...\n",
      "\n",
      "Character-level tokens (first 50): ['W', 'a', 'l', 'l', ' ', 'S', 't', '.', ' ', 'B', 'e', 'a', 'r', 's', ' ', 'C', 'l', 'a', 'w', ' ', 'B', 'a', 'c', 'k', ' ', 'I', 'n', 't', 'o', ' ', 't', 'h', 'e', ' ', 'B', 'l', 'a', 'c', 'k', ' ', '(', 'R', 'e', 'u', 't', 'e', 'r', 's', ')', ' ']\n",
      "\n",
      "Char 3-gram tokens (first 20): ['Wal', 'all', 'll ', 'l S', ' St', 'St.', 't. ', '. B', ' Be', 'Bea', 'ear', 'ars', 'rs ', 's C', ' Cl', 'Cla', 'law', 'aw ', 'w B', ' Ba']\n",
      "\n",
      "Total characters: 144\n",
      "Total char 3-grams: 142\n"
     ]
    }
   ],
   "source": [
    "def character_level_tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "def char_ngram_tokenize(text, n=3):\n",
    "    tokens = []\n",
    "    for i in range(len(text) - n + 1):\n",
    "        tokens.append(text[i:i+n])\n",
    "    return tokens\n",
    "\n",
    "sample_text = train_texts[0]\n",
    "char_tokens = character_level_tokenize(sample_text)\n",
    "char_3gram_tokens = char_ngram_tokenize(sample_text, n=3)\n",
    "\n",
    "print(f\"Original text: {sample_text[:100]}...\")\n",
    "print(f\"\\nCharacter-level tokens (first 50): {char_tokens[:50]}\")\n",
    "print(f\"\\nChar 3-gram tokens (first 20): {char_3gram_tokens[:20]}\")\n",
    "print(f\"\\nTotal characters: {len(char_tokens)}\")\n",
    "print(f\"Total char 3-grams: {len(char_3gram_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece токенизация (BERT)\n",
    "\n",
    "WordPiece — это субворд токенизатор, используемый в BERT и других моделях. Он разбивает слова на более мелкие части (subwords), что позволяет обрабатывать редкие слова и уменьшить размер словаря.\n",
    "\n",
    "**Принцип работы:** Использует жадный алгоритм для разбиения слов на субворды. Слова, которых нет в словаре, разбиваются на известные части, что минимизирует количество [UNK] токенов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n",
      "\n",
      "WordPiece tokens (39 tokens):\n",
      "['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', '\\\\', 'band', 'of', 'ultra', '-']\n",
      "\n",
      "Token IDs (first 30): [2813, 2358, 1012, 6468, 15020, 2067, 2046, 1996, 2304, 1006, 26665, 1007, 26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, 1040, 11101, 2989, 1032, 2316, 1997, 11087, 1011]\n",
      "\n",
      "Vocabulary size: 30522\n",
      "\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sample_text = train_texts[0]\n",
    "wordpiece_tokens = bert_tokenizer.tokenize(sample_text)\n",
    "wordpiece_ids = bert_tokenizer.encode(sample_text, add_special_tokens=False)\n",
    "\n",
    "print(f\"Original text: {sample_text[:150]}...\")\n",
    "print(f\"\\nWordPiece tokens ({len(wordpiece_tokens)} tokens):\")\n",
    "print(wordpiece_tokens[:30])\n",
    "print(f\"\\nToken IDs (first 30): {wordpiece_ids[:30]}\")\n",
    "print(f\"\\nVocabulary size: {bert_tokenizer.vocab_size}\")\n",
    "print(f\"\\nSpecial tokens: {bert_tokenizer.special_tokens_map}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Language Model токенизация (SentencePiece)\n",
    "\n",
    "Unigram LM — это вероятностная субворд модель, реализованная в SentencePiece. Она использует языковую модель для определения оптимального разбиения текста и поддерживает subword dropout для регуляризации.\n",
    "\n",
    "**Принцип работы:** Обучается на корпусе текста, вычисляя вероятности различных разбиений. Может работать с любым Unicode текстом без предобработки и поддерживает вероятностное разбиение.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n",
      "\n",
      "Unigram tokens (43 tokens):\n",
      "['▁Wall', '▁St', '.', '▁Bears', '▁C', 'law', '▁Back', '▁In', 'to', '▁the', '▁Black', '▁(', 'Reuters', ')', '▁Reuters', '▁-', '▁Short', '-', 's', 'ell', 'ers', ',', '▁Wall', '▁Street', \"'\", 's', '▁dwindling', '\\\\', 'band', '▁of']\n",
      "\n",
      "Token IDs (first 30): [634, 402, 4, 3932, 99, 4339, 1114, 118, 249, 5, 2977, 17, 40, 18, 146, 20, 5571, 13, 3, 690, 124, 6, 634, 678, 21, 3, 7836, 43, 2953, 10]\n",
      "\n",
      "Vocabulary size: 8000\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "model_prefix = \"unigram_model\"\n",
    "vocab_size = 8000\n",
    "\n",
    "if not os.path.exists(f\"{model_prefix}.model\"):\n",
    "    with open(\"train_texts_sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in train_texts[:10000]:\n",
    "            f.write(text + \"\\n\")\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=\"train_texts_sample.txt\",\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=1.0\n",
    "    )\n",
    "    \n",
    "    os.remove(\"train_texts_sample.txt\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "\n",
    "sample_text = train_texts[0]\n",
    "unigram_tokens = sp.encode(sample_text, out_type=str)\n",
    "unigram_ids = sp.encode(sample_text, out_type=int)\n",
    "\n",
    "print(f\"Original text: {sample_text[:150]}...\")\n",
    "print(f\"\\nUnigram tokens ({len(unigram_tokens)} tokens):\")\n",
    "print(unigram_tokens[:30])\n",
    "print(f\"\\nToken IDs (first 30): {unigram_ids[:30]}\")\n",
    "print(f\"\\nVocabulary size: {sp.get_piece_size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level BPE токенизация (GPT-2/tiktoken)\n",
    "\n",
    "Byte-level BPE — это токенизатор, используемый в GPT-2 и других современных языковых моделях. Он работает на уровне байтов, что делает его универсальным для любого Unicode текста и практически исключает появление неизвестных токенов [UNK].\n",
    "\n",
    "**Принцип работы:** Сначала текст кодируется в UTF-8 байты, затем применяется BPE (Byte Pair Encoding) для создания субворд токенов. Это позволяет обрабатывать любой текст без предобработки и минимизировать OOV (out-of-vocabulary) токены.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n",
      "\n",
      "Byte-level BPE tokens (37 tokens):\n",
      "Token strings (first 30): ['Wall', ' St', '.', ' Bears', ' Claw', ' Back', ' Into', ' the', ' Black', ' (', 'Reuters', ')', ' Reuters', ' -', ' Short', '-', 'sell', 'ers', ',', ' Wall', ' Street', \"'s\", ' dwindling', '\\\\', 'band', ' of', ' ultra', '-', 'cy', 'n']\n",
      "\n",
      "Token IDs (first 30): [22401, 520, 13, 15682, 30358, 5157, 20008, 262, 2619, 357, 12637, 8, 8428, 532, 10073, 12, 7255, 364, 11, 5007, 3530, 338, 45215, 59, 3903, 286, 14764, 12, 948, 77]\n",
      "\n",
      "Vocabulary size: 50257\n",
      "\n",
      "Decoded text matches original: True\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "sample_text = train_texts[0]\n",
    "bpe_tokens = gpt2_tokenizer.encode(sample_text)\n",
    "bpe_token_strings = [gpt2_tokenizer.decode([token]) for token in bpe_tokens[:30]]\n",
    "\n",
    "print(f\"Original text: {sample_text[:150]}...\")\n",
    "print(f\"\\nByte-level BPE tokens ({len(bpe_tokens)} tokens):\")\n",
    "print(f\"Token strings (first 30): {bpe_token_strings}\")\n",
    "print(f\"\\nToken IDs (first 30): {bpe_tokens[:30]}\")\n",
    "print(f\"\\nVocabulary size: {gpt2_tokenizer.n_vocab}\")\n",
    "\n",
    "decoded_text = gpt2_tokenizer.decode(bpe_tokens)\n",
    "print(f\"\\nDecoded text matches original: {decoded_text == sample_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение токенизаторов\n",
    "\n",
    "Сравним различные токенизаторы на одном примере текста:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Word-level: 24 tokens\n",
      "Tokens: ['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 's']\n",
      "\n",
      "Character-level: 144 tokens\n",
      "Tokens (first 50): ['W', 'a', 'l', 'l', ' ', 'S', 't', '.', ' ', 'B', 'e', 'a', 'r', 's', ' ', 'C', 'l', 'a', 'w', ' ', 'B', 'a', 'c', 'k', ' ', 'I', 'n', 't', 'o', ' ', 't', 'h', 'e', ' ', 'B', 'l', 'a', 'c', 'k', ' ', '(', 'R', 'e', 'u', 't', 'e', 'r', 's', ')', ' ']\n",
      "\n",
      "WordPiece (BERT): 39 tokens\n",
      "Tokens: ['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short']\n",
      "\n",
      "Unigram (SentencePiece): 43 tokens\n",
      "Tokens: ['▁Wall', '▁St', '.', '▁Bears', '▁C', 'law', '▁Back', '▁In', 'to', '▁the', '▁Black', '▁(', 'Reuters', ')', '▁Reuters']\n",
      "\n",
      "Byte-level BPE (GPT-2): 37 tokens\n",
      "Tokens: ['Wall', ' St', '.', ' Bears', ' Claw', ' Back', ' Into', ' the', ' Black', ' (', 'Reuters', ')', ' Reuters', ' -', ' Short']\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_texts[0]\n",
    "\n",
    "print(f\"Original text:\\n{sample_text}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "word_tokens = word_level_tokenize(sample_text)\n",
    "print(f\"\\nWord-level: {len(word_tokens)} tokens\")\n",
    "print(f\"Tokens: {word_tokens[:15]}\")\n",
    "\n",
    "char_tokens = character_level_tokenize(sample_text)\n",
    "print(f\"\\nCharacter-level: {len(char_tokens)} tokens\")\n",
    "print(f\"Tokens (first 50): {char_tokens[:50]}\")\n",
    "\n",
    "wordpiece_tokens = bert_tokenizer.tokenize(sample_text)\n",
    "print(f\"\\nWordPiece (BERT): {len(wordpiece_tokens)} tokens\")\n",
    "print(f\"Tokens: {wordpiece_tokens[:15]}\")\n",
    "\n",
    "unigram_tokens = sp.encode(sample_text, out_type=str)\n",
    "print(f\"\\nUnigram (SentencePiece): {len(unigram_tokens)} tokens\")\n",
    "print(f\"Tokens: {unigram_tokens[:15]}\")\n",
    "\n",
    "bpe_tokens = gpt2_tokenizer.encode(sample_text)\n",
    "bpe_token_strings = [gpt2_tokenizer.decode([token]) for token in bpe_tokens[:15]]\n",
    "print(f\"\\nByte-level BPE (GPT-2): {len(bpe_tokens)} tokens\")\n",
    "print(f\"Tokens: {bpe_token_strings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Векторизация текста\n",
    "\n",
    "После токенизации необходимо преобразовать тексты в числовые векторы. Рассмотрим различные методы векторизации для каждого типа токенизации: CountVectorizer, TF-IDF, HashingVectorizer и char n-граммы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level векторизация\n",
    "\n",
    "Для word-level токенизации используем CountVectorizer и TfidfVectorizer с n-граммами (1-2). Для больших словарей можно использовать HashingVectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level векторизация:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 0.90s\n",
      "\n",
      "TfidfVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 0.83s\n",
      "\n",
      "HashingVectorizer (1-2 grams, 2^18 features):\n",
      "  Train shape: (10000, 262144)\n",
      "  Test shape: (1000, 262144)\n",
      "  Time: 0.41s\n"
     ]
    }
   ],
   "source": [
    "train_subset = train_texts[:10000]\n",
    "train_labels_subset = train_labels[:10000]\n",
    "test_subset = test_texts[:1000]\n",
    "test_labels_subset = test_labels[:1000]\n",
    "\n",
    "print(\"Word-level векторизация:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "word_count_vectorizer = CountVectorizer(tokenizer=word_level_tokenize, ngram_range=(1, 2), max_features=50000)\n",
    "word_count_train = word_count_vectorizer.fit_transform(train_subset)\n",
    "word_count_test = word_count_vectorizer.transform(test_subset)\n",
    "word_count_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {word_count_train.shape}\")\n",
    "print(f\"  Test shape: {word_count_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(word_count_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {word_count_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "word_tfidf_vectorizer = TfidfVectorizer(tokenizer=word_level_tokenize, ngram_range=(1, 2), max_features=50000)\n",
    "word_tfidf_train = word_tfidf_vectorizer.fit_transform(train_subset)\n",
    "word_tfidf_test = word_tfidf_vectorizer.transform(test_subset)\n",
    "word_tfidf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {word_tfidf_train.shape}\")\n",
    "print(f\"  Test shape: {word_tfidf_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(word_tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {word_tfidf_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "word_hash_vectorizer = HashingVectorizer(tokenizer=word_level_tokenize, ngram_range=(1, 2), n_features=2**18)\n",
    "word_hash_train = word_hash_vectorizer.transform(train_subset)\n",
    "word_hash_test = word_hash_vectorizer.transform(test_subset)\n",
    "word_hash_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHashingVectorizer (1-2 grams, 2^18 features):\")\n",
    "print(f\"  Train shape: {word_hash_train.shape}\")\n",
    "print(f\"  Test shape: {word_hash_test.shape}\")\n",
    "print(f\"  Time: {word_hash_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества Word-level векторов:\n",
      "============================================================\n",
      "CountVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8780\n",
      "  Training time: 15.73s\n",
      "\n",
      "TfidfVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8930\n",
      "  Training time: 10.95s\n",
      "\n",
      "HashingVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8570\n",
      "  Training time: 21.10s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nОценка качества Word-level векторов:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_count = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_count.fit(word_count_train, train_labels_subset)\n",
    "count_pred = lr_count.predict(word_count_test)\n",
    "count_acc = accuracy_score(test_labels_subset, count_pred)\n",
    "count_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {count_acc:.4f}\")\n",
    "print(f\"  Training time: {count_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_tfidf = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_tfidf.fit(word_tfidf_train, train_labels_subset)\n",
    "tfidf_pred = lr_tfidf.predict(word_tfidf_test)\n",
    "tfidf_acc = accuracy_score(test_labels_subset, tfidf_pred)\n",
    "tfidf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {tfidf_acc:.4f}\")\n",
    "print(f\"  Training time: {tfidf_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_hash = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_hash.fit(word_hash_train, train_labels_subset)\n",
    "hash_pred = lr_hash.predict(word_hash_test)\n",
    "hash_acc = accuracy_score(test_labels_subset, hash_pred)\n",
    "hash_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHashingVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {hash_acc:.4f}\")\n",
    "print(f\"  Training time: {hash_train_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level векторизация\n",
    "\n",
    "Для character-level токенизации используем char n-граммы (3-5), что особенно эффективно для коротких и шумных текстов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level векторизация:\n",
      "============================================================\n",
      "CountVectorizer (char 3-5 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 3.81s\n",
      "\n",
      "TfidfVectorizer (char 3-5 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 4.09s\n",
      "\n",
      "HashingVectorizer (char 3-5 grams, 2^18 features):\n",
      "  Train shape: (10000, 262144)\n",
      "  Test shape: (1000, 262144)\n",
      "  Time: 2.54s\n"
     ]
    }
   ],
   "source": [
    "print(\"Character-level векторизация:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "char_count_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 5), max_features=50000)\n",
    "char_count_train = char_count_vectorizer.fit_transform(train_subset)\n",
    "char_count_test = char_count_vectorizer.transform(test_subset)\n",
    "char_count_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer (char 3-5 grams):\")\n",
    "print(f\"  Train shape: {char_count_train.shape}\")\n",
    "print(f\"  Test shape: {char_count_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(char_count_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {char_count_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "char_tfidf_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=50000)\n",
    "char_tfidf_train = char_tfidf_vectorizer.fit_transform(train_subset)\n",
    "char_tfidf_test = char_tfidf_vectorizer.transform(test_subset)\n",
    "char_tfidf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer (char 3-5 grams):\")\n",
    "print(f\"  Train shape: {char_tfidf_train.shape}\")\n",
    "print(f\"  Test shape: {char_tfidf_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(char_tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {char_tfidf_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "char_hash_vectorizer = HashingVectorizer(analyzer='char', ngram_range=(3, 5), n_features=2**18)\n",
    "char_hash_train = char_hash_vectorizer.transform(train_subset)\n",
    "char_hash_test = char_hash_vectorizer.transform(test_subset)\n",
    "char_hash_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHashingVectorizer (char 3-5 grams, 2^18 features):\")\n",
    "print(f\"  Train shape: {char_hash_train.shape}\")\n",
    "print(f\"  Test shape: {char_hash_test.shape}\")\n",
    "print(f\"  Time: {char_hash_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества Character-level векторов:\n",
      "============================================================\n",
      "CountVectorizer (char) + LogisticRegression:\n",
      "  Accuracy: 0.8680\n",
      "  Training time: 16.59s\n",
      "\n",
      "TfidfVectorizer (char) + LogisticRegression:\n",
      "  Accuracy: 0.8890\n",
      "  Training time: 12.11s\n",
      "\n",
      "HashingVectorizer (char) + LogisticRegression:\n",
      "  Accuracy: 0.8720\n",
      "  Training time: 23.94s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nОценка качества Character-level векторов:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_char_count = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_char_count.fit(char_count_train, train_labels_subset)\n",
    "char_count_pred = lr_char_count.predict(char_count_test)\n",
    "char_count_acc = accuracy_score(test_labels_subset, char_count_pred)\n",
    "char_count_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer (char) + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {char_count_acc:.4f}\")\n",
    "print(f\"  Training time: {char_count_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_char_tfidf = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_char_tfidf.fit(char_tfidf_train, train_labels_subset)\n",
    "char_tfidf_pred = lr_char_tfidf.predict(char_tfidf_test)\n",
    "char_tfidf_acc = accuracy_score(test_labels_subset, char_tfidf_pred)\n",
    "char_tfidf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer (char) + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {char_tfidf_acc:.4f}\")\n",
    "print(f\"  Training time: {char_tfidf_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_char_hash = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_char_hash.fit(char_hash_train, train_labels_subset)\n",
    "char_hash_pred = lr_char_hash.predict(char_hash_test)\n",
    "char_hash_acc = accuracy_score(test_labels_subset, char_hash_pred)\n",
    "char_hash_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHashingVectorizer (char) + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {char_hash_acc:.4f}\")\n",
    "print(f\"  Training time: {char_hash_train_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece векторизация\n",
    "\n",
    "Для WordPiece токенов используем CountVectorizer и TfidfVectorizer. Токены уже получены через BERT токенизатор, поэтому используем их напрямую.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece векторизация:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 2.82s\n",
      "\n",
      "TfidfVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 2.86s\n"
     ]
    }
   ],
   "source": [
    "def wordpiece_tokenize_text(text):\n",
    "    return bert_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"WordPiece векторизация:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "wordpiece_count_vectorizer = CountVectorizer(tokenizer=wordpiece_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "wordpiece_count_train = wordpiece_count_vectorizer.fit_transform(train_subset)\n",
    "wordpiece_count_test = wordpiece_count_vectorizer.transform(test_subset)\n",
    "wordpiece_count_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {wordpiece_count_train.shape}\")\n",
    "print(f\"  Test shape: {wordpiece_count_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(wordpiece_count_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {wordpiece_count_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "wordpiece_tfidf_vectorizer = TfidfVectorizer(tokenizer=wordpiece_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "wordpiece_tfidf_train = wordpiece_tfidf_vectorizer.fit_transform(train_subset)\n",
    "wordpiece_tfidf_test = wordpiece_tfidf_vectorizer.transform(test_subset)\n",
    "wordpiece_tfidf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {wordpiece_tfidf_train.shape}\")\n",
    "print(f\"  Test shape: {wordpiece_tfidf_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(wordpiece_tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {wordpiece_tfidf_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества WordPiece векторов:\n",
      "============================================================\n",
      "CountVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8840\n",
      "  Training time: 17.19s\n",
      "\n",
      "TfidfVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8920\n",
      "  Training time: 13.50s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nОценка качества WordPiece векторов:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_wordpiece_count = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_wordpiece_count.fit(wordpiece_count_train, train_labels_subset)\n",
    "wordpiece_count_pred = lr_wordpiece_count.predict(wordpiece_count_test)\n",
    "wordpiece_count_acc = accuracy_score(test_labels_subset, wordpiece_count_pred)\n",
    "wordpiece_count_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {wordpiece_count_acc:.4f}\")\n",
    "print(f\"  Training time: {wordpiece_count_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_wordpiece_tfidf = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_wordpiece_tfidf.fit(wordpiece_tfidf_train, train_labels_subset)\n",
    "wordpiece_tfidf_pred = lr_wordpiece_tfidf.predict(wordpiece_tfidf_test)\n",
    "wordpiece_tfidf_acc = accuracy_score(test_labels_subset, wordpiece_tfidf_pred)\n",
    "wordpiece_tfidf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {wordpiece_tfidf_acc:.4f}\")\n",
    "print(f\"  Training time: {wordpiece_tfidf_train_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram (SentencePiece) векторизация\n",
    "\n",
    "Для Unigram токенов из SentencePiece используем стандартные векторизаторы с токенами, полученными через обученную модель.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (SentencePiece) векторизация:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 1.54s\n",
      "\n",
      "TfidfVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 1.59s\n"
     ]
    }
   ],
   "source": [
    "def unigram_tokenize_text(text):\n",
    "    return sp.encode(text, out_type=str)\n",
    "\n",
    "print(\"Unigram (SentencePiece) векторизация:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "unigram_count_vectorizer = CountVectorizer(tokenizer=unigram_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "unigram_count_train = unigram_count_vectorizer.fit_transform(train_subset)\n",
    "unigram_count_test = unigram_count_vectorizer.transform(test_subset)\n",
    "unigram_count_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {unigram_count_train.shape}\")\n",
    "print(f\"  Test shape: {unigram_count_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(unigram_count_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {unigram_count_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(tokenizer=unigram_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "unigram_tfidf_train = unigram_tfidf_vectorizer.fit_transform(train_subset)\n",
    "unigram_tfidf_test = unigram_tfidf_vectorizer.transform(test_subset)\n",
    "unigram_tfidf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {unigram_tfidf_train.shape}\")\n",
    "print(f\"  Test shape: {unigram_tfidf_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(unigram_tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {unigram_tfidf_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества Unigram векторов:\n",
      "============================================================\n",
      "CountVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8770\n",
      "  Training time: 19.65s\n",
      "\n",
      "TfidfVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8870\n",
      "  Training time: 14.48s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nОценка качества Unigram векторов:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_unigram_count = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_unigram_count.fit(unigram_count_train, train_labels_subset)\n",
    "unigram_count_pred = lr_unigram_count.predict(unigram_count_test)\n",
    "unigram_count_acc = accuracy_score(test_labels_subset, unigram_count_pred)\n",
    "unigram_count_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {unigram_count_acc:.4f}\")\n",
    "print(f\"  Training time: {unigram_count_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_unigram_tfidf = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_unigram_tfidf.fit(unigram_tfidf_train, train_labels_subset)\n",
    "unigram_tfidf_pred = lr_unigram_tfidf.predict(unigram_tfidf_test)\n",
    "unigram_tfidf_acc = accuracy_score(test_labels_subset, unigram_tfidf_pred)\n",
    "unigram_tfidf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {unigram_tfidf_acc:.4f}\")\n",
    "print(f\"  Training time: {unigram_tfidf_train_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level BPE векторизация\n",
    "\n",
    "Для Byte-level BPE токенов используем векторизаторы с токенами, полученными через tiktoken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-level BPE векторизация:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 1.79s\n",
      "\n",
      "TfidfVectorizer (1-2 grams):\n",
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 1.70s\n"
     ]
    }
   ],
   "source": [
    "def bpe_tokenize_text(text):\n",
    "    tokens = gpt2_tokenizer.encode(text)\n",
    "    return [gpt2_tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "print(\"Byte-level BPE векторизация:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "bpe_count_vectorizer = CountVectorizer(tokenizer=bpe_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "bpe_count_train = bpe_count_vectorizer.fit_transform(train_subset)\n",
    "bpe_count_test = bpe_count_vectorizer.transform(test_subset)\n",
    "bpe_count_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {bpe_count_train.shape}\")\n",
    "print(f\"  Test shape: {bpe_count_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(bpe_count_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {bpe_count_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "bpe_tfidf_vectorizer = TfidfVectorizer(tokenizer=bpe_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "bpe_tfidf_train = bpe_tfidf_vectorizer.fit_transform(train_subset)\n",
    "bpe_tfidf_test = bpe_tfidf_vectorizer.transform(test_subset)\n",
    "bpe_tfidf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer (1-2 grams):\")\n",
    "print(f\"  Train shape: {bpe_tfidf_train.shape}\")\n",
    "print(f\"  Test shape: {bpe_tfidf_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(bpe_tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Time: {bpe_tfidf_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества Byte-level BPE векторов:\n",
      "============================================================\n",
      "CountVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.8880\n",
      "  Training time: 14.74s\n",
      "\n",
      "TfidfVectorizer + LogisticRegression:\n",
      "  Accuracy: 0.9020\n",
      "  Training time: 9.28s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nОценка качества Byte-level BPE векторов:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_bpe_count = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_bpe_count.fit(bpe_count_train, train_labels_subset)\n",
    "bpe_count_pred = lr_bpe_count.predict(bpe_count_test)\n",
    "bpe_count_acc = accuracy_score(test_labels_subset, bpe_count_pred)\n",
    "bpe_count_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CountVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {bpe_count_acc:.4f}\")\n",
    "print(f\"  Training time: {bpe_count_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_bpe_tfidf = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_bpe_tfidf.fit(bpe_tfidf_train, train_labels_subset)\n",
    "bpe_tfidf_pred = lr_bpe_tfidf.predict(bpe_tfidf_test)\n",
    "bpe_tfidf_acc = accuracy_score(test_labels_subset, bpe_tfidf_pred)\n",
    "bpe_tfidf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTfidfVectorizer + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {bpe_tfidf_acc:.4f}\")\n",
    "print(f\"  Training time: {bpe_tfidf_train_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение всех методов векторизации\n",
    "\n",
    "Сводная таблица результатов по времени векторизации и качеству классификации:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сводная таблица результатов:\n",
      "================================================================================\n",
      "    Токенизатор             Векторизатор  Время векторизации  Accuracy  Время обучения\n",
      " Byte-level BPE          TfidfVectorizer            1.702362     0.902        9.276575\n",
      "     Word-level          TfidfVectorizer            0.831401     0.893       10.953832\n",
      "      WordPiece          TfidfVectorizer            2.855483     0.892       13.501026\n",
      "Character-level   TfidfVectorizer (char)            4.094401     0.889       12.109068\n",
      " Byte-level BPE          CountVectorizer            1.788724     0.888       14.740647\n",
      "        Unigram          TfidfVectorizer            1.589704     0.887       14.481398\n",
      "      WordPiece          CountVectorizer            2.819031     0.884       17.188610\n",
      "     Word-level          CountVectorizer            0.904554     0.878       15.725384\n",
      "        Unigram          CountVectorizer            1.543808     0.877       19.651316\n",
      "Character-level HashingVectorizer (char)            2.535200     0.872       23.943213\n",
      "Character-level   CountVectorizer (char)            3.812647     0.868       16.589035\n",
      "     Word-level        HashingVectorizer            0.408046     0.857       21.095006\n",
      "\n",
      "\n",
      "Лучшие результаты по Accuracy:\n",
      "================================================================================\n",
      "   Токенизатор    Векторизатор  Время векторизации  Accuracy  Время обучения\n",
      "Byte-level BPE TfidfVectorizer            1.702362     0.902        9.276575\n",
      "    Word-level TfidfVectorizer            0.831401     0.893       10.953832\n",
      "     WordPiece TfidfVectorizer            2.855483     0.892       13.501026\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = [\n",
    "    {'Токенизатор': 'Word-level', 'Векторизатор': 'CountVectorizer', \n",
    "     'Время векторизации': word_count_time, 'Accuracy': count_acc, 'Время обучения': count_train_time},\n",
    "    {'Токенизатор': 'Word-level', 'Векторизатор': 'TfidfVectorizer', \n",
    "     'Время векторизации': word_tfidf_time, 'Accuracy': tfidf_acc, 'Время обучения': tfidf_train_time},\n",
    "    {'Токенизатор': 'Word-level', 'Векторизатор': 'HashingVectorizer', \n",
    "     'Время векторизации': word_hash_time, 'Accuracy': hash_acc, 'Время обучения': hash_train_time},\n",
    "    {'Токенизатор': 'Character-level', 'Векторизатор': 'CountVectorizer (char)', \n",
    "     'Время векторизации': char_count_time, 'Accuracy': char_count_acc, 'Время обучения': char_count_train_time},\n",
    "    {'Токенизатор': 'Character-level', 'Векторизатор': 'TfidfVectorizer (char)', \n",
    "     'Время векторизации': char_tfidf_time, 'Accuracy': char_tfidf_acc, 'Время обучения': char_tfidf_train_time},\n",
    "    {'Токенизатор': 'Character-level', 'Векторизатор': 'HashingVectorizer (char)', \n",
    "     'Время векторизации': char_hash_time, 'Accuracy': char_hash_acc, 'Время обучения': char_hash_train_time},\n",
    "    {'Токенизатор': 'WordPiece', 'Векторизатор': 'CountVectorizer', \n",
    "     'Время векторизации': wordpiece_count_time, 'Accuracy': wordpiece_count_acc, 'Время обучения': wordpiece_count_train_time},\n",
    "    {'Токенизатор': 'WordPiece', 'Векторизатор': 'TfidfVectorizer', \n",
    "     'Время векторизации': wordpiece_tfidf_time, 'Accuracy': wordpiece_tfidf_acc, 'Время обучения': wordpiece_tfidf_train_time},\n",
    "    {'Токенизатор': 'Unigram', 'Векторизатор': 'CountVectorizer', \n",
    "     'Время векторизации': unigram_count_time, 'Accuracy': unigram_count_acc, 'Время обучения': unigram_count_train_time},\n",
    "    {'Токенизатор': 'Unigram', 'Векторизатор': 'TfidfVectorizer', \n",
    "     'Время векторизации': unigram_tfidf_time, 'Accuracy': unigram_tfidf_acc, 'Время обучения': unigram_tfidf_train_time},\n",
    "    {'Токенизатор': 'Byte-level BPE', 'Векторизатор': 'CountVectorizer', \n",
    "     'Время векторизации': bpe_count_time, 'Accuracy': bpe_count_acc, 'Время обучения': bpe_count_train_time},\n",
    "    {'Токенизатор': 'Byte-level BPE', 'Векторизатор': 'TfidfVectorizer', \n",
    "     'Время векторизации': bpe_tfidf_time, 'Accuracy': bpe_tfidf_acc, 'Время обучения': bpe_tfidf_train_time},\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"Сводная таблица результатов:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nЛучшие результаты по Accuracy:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_results.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: Сравнение TF-IDF и BM25 на Byte-level BPE\n",
    "\n",
    "BM25 (Best Matching 25) — это улучшенная версия TF-IDF, которая лучше учитывает длину документа и использует нелинейную нормализацию частоты терминов. Сравним TF-IDF и BM25 на byte-level BPE токенах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 векторизация\n",
    "\n",
    "BM25 использует формулу с параметрами k1 и b для нормализации частоты терминов и длины документа. Это делает его более устойчивым к различиям в длине документов по сравнению с TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "\n",
    "class BM25Vectorizer:\n",
    "    def __init__(self, tokenizer, k1=1.5, b=0.75):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.bm25 = None\n",
    "        self.vocab = None\n",
    "        self.idf = None\n",
    "        self.avgdl = None\n",
    "        self.tokenized_corpus = None\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        self.tokenized_corpus = [self.tokenizer(text) for text in texts]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus, k1=self.k1, b=self.b)\n",
    "        \n",
    "        all_tokens = set()\n",
    "        for tokens in self.tokenized_corpus:\n",
    "            all_tokens.update(tokens)\n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted(all_tokens))}\n",
    "        \n",
    "        doc_freqs = Counter()\n",
    "        for tokens in self.tokenized_corpus:\n",
    "            doc_freqs.update(set(tokens))\n",
    "        \n",
    "        n_docs = len(self.tokenized_corpus)\n",
    "        self.idf = {token: np.log((n_docs - doc_freqs[token] + 0.5) / (doc_freqs[token] + 0.5) + 1.0) \n",
    "                   for token in self.vocab}\n",
    "        \n",
    "        self.avgdl = np.mean([len(tokens) for tokens in self.tokenized_corpus])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _bm25_score(self, token, tf, doc_len):\n",
    "        if token not in self.idf:\n",
    "            return 0.0\n",
    "        idf = self.idf[token]\n",
    "        norm_tf = (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl))\n",
    "        return idf * norm_tf\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        tokenized_texts = [self.tokenizer(text) for text in texts]\n",
    "        n_docs = len(texts)\n",
    "        n_features = len(self.vocab)\n",
    "        \n",
    "        rows, cols, data = [], [], []\n",
    "        \n",
    "        for doc_idx, tokens in enumerate(tokenized_texts):\n",
    "            doc_len = len(tokens)\n",
    "            token_counts = Counter(tokens)\n",
    "            \n",
    "            for token, tf in token_counts.items():\n",
    "                if token in self.vocab:\n",
    "                    feature_idx = self.vocab[token]\n",
    "                    score = self._bm25_score(token, tf, doc_len)\n",
    "                    rows.append(doc_idx)\n",
    "                    cols.append(feature_idx)\n",
    "                    data.append(score)\n",
    "        \n",
    "        return sp.csr_matrix((data, (rows, cols)), shape=(n_docs, n_features))\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        return self.fit(texts).transform(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сравнение TF-IDF и BM25 на Byte-level BPE:\n",
      "============================================================\n",
      "\n",
      "TF-IDF векторизация:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam2511/venvs/train_py10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train shape: (10000, 50000)\n",
      "  Test shape: (1000, 50000)\n",
      "  Vocabulary size: 50000\n",
      "  Time: 1.75s\n",
      "\n",
      "BM25 векторизация:\n",
      "  Train shape: (10000, 26128)\n",
      "  Test shape: (1000, 26128)\n",
      "  Vocabulary size: 26128\n",
      "  Time: 2.28s\n"
     ]
    }
   ],
   "source": [
    "print(\"Сравнение TF-IDF и BM25 на Byte-level BPE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nTF-IDF векторизация:\")\n",
    "start_time = time.time()\n",
    "bpe_tfidf_vectorizer_full = TfidfVectorizer(tokenizer=bpe_tokenize_text, ngram_range=(1, 2), max_features=50000)\n",
    "bpe_tfidf_train_full = bpe_tfidf_vectorizer_full.fit_transform(train_subset)\n",
    "bpe_tfidf_test_full = bpe_tfidf_vectorizer_full.transform(test_subset)\n",
    "tfidf_vectorization_time = time.time() - start_time\n",
    "\n",
    "print(f\"  Train shape: {bpe_tfidf_train_full.shape}\")\n",
    "print(f\"  Test shape: {bpe_tfidf_test_full.shape}\")\n",
    "print(f\"  Vocabulary size: {len(bpe_tfidf_vectorizer_full.vocabulary_)}\")\n",
    "print(f\"  Time: {tfidf_vectorization_time:.2f}s\")\n",
    "\n",
    "print(\"\\nBM25 векторизация:\")\n",
    "start_time = time.time()\n",
    "bpe_bm25_vectorizer = BM25Vectorizer(tokenizer=bpe_tokenize_text, k1=1.5, b=0.75)\n",
    "bpe_bm25_train = bpe_bm25_vectorizer.fit_transform(train_subset)\n",
    "bpe_bm25_test = bpe_bm25_vectorizer.transform(test_subset)\n",
    "bm25_vectorization_time = time.time() - start_time\n",
    "\n",
    "print(f\"  Train shape: {bpe_bm25_train.shape}\")\n",
    "print(f\"  Test shape: {bpe_bm25_test.shape}\")\n",
    "print(f\"  Vocabulary size: {len(bpe_bm25_vectorizer.vocab)}\")\n",
    "print(f\"  Time: {bm25_vectorization_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества TF-IDF и BM25:\n",
      "============================================================\n",
      "TF-IDF + LogisticRegression:\n",
      "  Accuracy: 0.9020\n",
      "  Training time: 9.20s\n",
      "\n",
      "BM25 + LogisticRegression:\n",
      "  Accuracy: 0.8940\n",
      "  Training time: 4.64s\n",
      "\n",
      "Разница в Accuracy: 0.0080\n",
      "TF-IDF лучше на 0.80%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nОценка качества TF-IDF и BM25:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "lr_tfidf_bpe = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_tfidf_bpe.fit(bpe_tfidf_train_full, train_labels_subset)\n",
    "tfidf_bpe_pred = lr_tfidf_bpe.predict(bpe_tfidf_test_full)\n",
    "tfidf_bpe_acc = accuracy_score(test_labels_subset, tfidf_bpe_pred)\n",
    "tfidf_bpe_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"TF-IDF + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {tfidf_bpe_acc:.4f}\")\n",
    "print(f\"  Training time: {tfidf_bpe_train_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_bm25_bpe = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_bm25_bpe.fit(bpe_bm25_train, train_labels_subset)\n",
    "bm25_bpe_pred = lr_bm25_bpe.predict(bpe_bm25_test)\n",
    "bm25_bpe_acc = accuracy_score(test_labels_subset, bm25_bpe_pred)\n",
    "bm25_bpe_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBM25 + LogisticRegression:\")\n",
    "print(f\"  Accuracy: {bm25_bpe_acc:.4f}\")\n",
    "print(f\"  Training time: {bm25_bpe_train_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nРазница в Accuracy: {abs(tfidf_bpe_acc - bm25_bpe_acc):.4f}\")\n",
    "if bm25_bpe_acc > tfidf_bpe_acc:\n",
    "    print(f\"BM25 лучше на {(bm25_bpe_acc - tfidf_bpe_acc) * 100:.2f}%\")\n",
    "else:\n",
    "    print(f\"TF-IDF лучше на {(tfidf_bpe_acc - bm25_bpe_acc) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнительная таблица TF-IDF vs BM25:\n",
      "================================================================================\n",
      " Метод  Время векторизации  Accuracy  Время обучения  Общее время\n",
      "TF-IDF            1.754680     0.902        9.201822    10.956502\n",
      "  BM25            2.284903     0.894        4.639738     6.924641\n"
     ]
    }
   ],
   "source": [
    "comparison_results = pd.DataFrame([\n",
    "    {\n",
    "        'Метод': 'TF-IDF',\n",
    "        'Время векторизации': tfidf_vectorization_time,\n",
    "        'Accuracy': tfidf_bpe_acc,\n",
    "        'Время обучения': tfidf_bpe_train_time,\n",
    "        'Общее время': tfidf_vectorization_time + tfidf_bpe_train_time\n",
    "    },\n",
    "    {\n",
    "        'Метод': 'BM25',\n",
    "        'Время векторизации': bm25_vectorization_time,\n",
    "        'Accuracy': bm25_bpe_acc,\n",
    "        'Время обучения': bm25_bpe_train_time,\n",
    "        'Общее время': bm25_vectorization_time + bm25_bpe_train_time\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nСравнительная таблица TF-IDF vs BM25:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5: Сравнение классических моделей ML на TF-IDF (Byte-level BPE)\n",
    "\n",
    "Сравним различные классические модели машинного обучения из sklearn на TF-IDF векторах с byte-level BPE токенизацией. Это позволит оценить, какая модель лучше работает с разреженными представлениями текста.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение различных моделей\n",
    "\n",
    "Обучим различные классификаторы на TF-IDF векторах с byte-level BPE и сравним их производительность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сравнение моделей sklearn на TF-IDF (Byte-level BPE):\n",
      "================================================================================\n",
      "\n",
      "LogisticRegression:\n",
      "  Accuracy: 0.9020\n",
      "  Время обучения: 9.33s\n",
      "  Время предсказания: 0.0013s\n",
      "\n",
      "MultinomialNB:\n",
      "  Accuracy: 0.8870\n",
      "  Время обучения: 0.01s\n",
      "  Время предсказания: 0.0011s\n",
      "\n",
      "LinearSVC:\n",
      "  Accuracy: 0.9140\n",
      "  Время обучения: 0.30s\n",
      "  Время предсказания: 0.0009s\n",
      "\n",
      "SGDClassifier:\n",
      "  Accuracy: 0.9120\n",
      "  Время обучения: 0.15s\n",
      "  Время предсказания: 0.0014s\n",
      "\n",
      "RandomForest:\n",
      "  Accuracy: 0.7690\n",
      "  Время обучения: 0.33s\n",
      "  Время предсказания: 0.0531s\n",
      "\n",
      "GradientBoosting:\n",
      "  Accuracy: 0.8510\n",
      "  Время обучения: 222.84s\n",
      "  Время предсказания: 0.0083s\n",
      "\n",
      "DecisionTree:\n",
      "  Accuracy: 0.6090\n",
      "  Время обучения: 2.75s\n",
      "  Время предсказания: 0.0009s\n"
     ]
    }
   ],
   "source": [
    "print(\"Сравнение моделей sklearn на TF-IDF (Byte-level BPE):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=500, random_state=42),\n",
    "    'MultinomialNB': MultinomialNB(alpha=1.0),\n",
    "    'LinearSVC': LinearSVC(max_iter=1000, random_state=42),\n",
    "    'SGDClassifier': SGDClassifier(max_iter=1000, random_state=42, loss='hinge'),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
    "}\n",
    "\n",
    "results_models = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    start_time = time.time()\n",
    "    model.fit(bpe_tfidf_train_full, train_labels_subset)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pred = model.predict(bpe_tfidf_test_full)\n",
    "    pred_time = time.time() - start_time\n",
    "    \n",
    "    acc = accuracy_score(test_labels_subset, pred)\n",
    "    \n",
    "    results_models.append({\n",
    "        'Модель': name,\n",
    "        'Accuracy': acc,\n",
    "        'Время обучения': train_time,\n",
    "        'Время предсказания': pred_time,\n",
    "        'Общее время': train_time + pred_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Время обучения: {train_time:.2f}s\")\n",
    "    print(f\"  Время предсказания: {pred_time:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Сводная таблица результатов моделей:\n",
      "================================================================================\n",
      "            Модель  Accuracy  Время обучения  Время предсказания  Общее время\n",
      "         LinearSVC     0.914        0.301154            0.000929     0.302083\n",
      "     SGDClassifier     0.912        0.145760            0.001358     0.147117\n",
      "LogisticRegression     0.902        9.328824            0.001313     9.330137\n",
      "     MultinomialNB     0.887        0.007668            0.001107     0.008775\n",
      "  GradientBoosting     0.851      222.835377            0.008273   222.843650\n",
      "      RandomForest     0.769        0.331515            0.053051     0.384566\n",
      "      DecisionTree     0.609        2.753665            0.000914     2.754579\n",
      "\n",
      "\n",
      "Топ-3 модели по Accuracy:\n",
      "================================================================================\n",
      "            Модель  Accuracy  Время обучения  Время предсказания  Общее время\n",
      "         LinearSVC     0.914        0.301154            0.000929     0.302083\n",
      "     SGDClassifier     0.912        0.145760            0.001358     0.147117\n",
      "LogisticRegression     0.902        9.328824            0.001313     9.330137\n"
     ]
    }
   ],
   "source": [
    "df_models = pd.DataFrame(results_models)\n",
    "df_models = df_models.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\\nСводная таблица результатов моделей:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_models.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nТоп-3 модели по Accuracy:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_models.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Детальный анализ лучших моделей\n",
    "\n",
    "Посмотрим детальные метрики для лучших моделей:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Детальные метрики для топ-3 моделей:\n",
      "================================================================================\n",
      "\n",
      "LinearSVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.95      0.91      0.93       268\n",
      "      Sports       0.95      0.96      0.96       274\n",
      "    Business       0.86      0.86      0.86       205\n",
      "    Sci/Tech       0.88      0.91      0.90       253\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.91      0.91      0.91      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n",
      "\n",
      "SGDClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.95      0.90      0.92       268\n",
      "      Sports       0.95      0.96      0.96       274\n",
      "    Business       0.86      0.85      0.86       205\n",
      "    Sci/Tech       0.88      0.91      0.90       253\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.91      0.91      0.91      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n",
      "\n",
      "LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.94      0.89      0.92       268\n",
      "      Sports       0.93      0.96      0.94       274\n",
      "    Business       0.88      0.83      0.85       205\n",
      "    Sci/Tech       0.85      0.91      0.88       253\n",
      "\n",
      "    accuracy                           0.90      1000\n",
      "   macro avg       0.90      0.90      0.90      1000\n",
      "weighted avg       0.90      0.90      0.90      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_3_models = df_models.head(3)['Модель'].values\n",
    "\n",
    "print(\"Детальные метрики для топ-3 моделей:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    model = models[model_name]\n",
    "    model.fit(bpe_tfidf_train_full, train_labels_subset)\n",
    "    pred = model.predict(bpe_tfidf_test_full)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(classification_report(test_labels_subset, pred, \n",
    "                              target_names=dataset['train'].features['label'].names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
